{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (302, 1) (2357111, 133) (2357111, 1) (2357111, 2)\n",
      "val (101, 1) (771095, 133) (771095, 1) (771095, 2)\n",
      "test (101, 1) (753623, 133) (753623, 1) (753623, 2)\n"
     ]
    }
   ],
   "source": [
    "def normalize_types(df):\n",
    "    for column in df.columns[df.dtypes == 'float64']:\n",
    "        df[column] = df[column].astype(np.float32)\n",
    "\n",
    "id = 1\n",
    "X_train = pd.read_csv(f'./data/X_train_{id}.csv', dtype=np.float32).values\n",
    "X_val = pd.read_csv(f'./data/X_val_{id}.csv', dtype=np.float32).values\n",
    "X_test = pd.read_csv(f'./data/X_test_{id}.csv', dtype=np.float32).values\n",
    "\n",
    "y_train = pd.read_csv(f'./data/y_train_{id}.csv', dtype=np.float32).values\n",
    "y_val = pd.read_csv(f'./data/y_val_{id}.csv', dtype=np.float32).values\n",
    "y_test = pd.read_csv(f'./data/y_test_{id}.csv', dtype=np.float32).values\n",
    "\n",
    "ids_train = pd.read_csv(f'./data/ids_train_{id}.csv').values\n",
    "ids_val = pd.read_csv(f'./data/ids_val_{id}.csv').values\n",
    "ids_test = pd.read_csv(f'./data/ids_test_{id}.csv').values\n",
    "\n",
    "buildings_train = pd.read_csv(f'./data/buildings_train_{id}.csv').values\n",
    "buildings_val = pd.read_csv(f'./data/buildings_val_{id}.csv').values\n",
    "buildings_test = pd.read_csv(f'./data/buildings_test_{id}.csv').values\n",
    "\n",
    "print('train', buildings_train.shape, X_train.shape, y_train.shape, ids_train.shape)\n",
    "print('val', buildings_val.shape, X_val.shape, y_val.shape, ids_val.shape)\n",
    "print('test', buildings_test.shape, X_test.shape, y_test.shape, ids_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (2347011, 6, 133) (2347011, 1)\n",
      "val (761900, 6, 133) (761900, 1)\n",
      "test (744438, 6, 133) (744438, 1)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(X, y, ids, buildings, time_steps=24):\n",
    "    X_output = []\n",
    "    y_output = []\n",
    "\n",
    "    for building in buildings:\n",
    "        X_values = X[ids[:,1] == building]\n",
    "        y_values = y[ids[:,1] == building]\n",
    "\n",
    "        for i in range(len(X_values) - time_steps + 1):\n",
    "            X_output.append(X_values[i : (i + time_steps)])\n",
    "            y_output.append(y_values[i])\n",
    "\n",
    "    return np.stack(X_output), np.array(y_output)\n",
    "\n",
    "time_step = 6\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, ids_train, buildings_train, time_step)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, ids_val, buildings_val, time_step)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, ids_test, buildings_test, time_step)\n",
    "\n",
    "print('train', X_train_seq.shape, y_train_seq.shape)\n",
    "print('val', X_val_seq.shape, y_val_seq.shape)\n",
    "print('test', X_test_seq.shape, y_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_results(model, history, test_eval, name):\n",
    "    path = f'./5_hyperparameters_3_models/{name}'\n",
    "    \n",
    "    model.save(f'{path}/model')\n",
    "\n",
    "    history_to_save = {}\n",
    "    history_to_save['train'] = history\n",
    "    history_to_save['test'] = test_eval\n",
    "\n",
    "    with open(f\"{path}/history.json\", \"w\") as history_file:\n",
    "        history_to_save_json = json.dumps(history_to_save, indent=4)\n",
    "        history_file.write(history_to_save_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>activation</th>\n",
       "      <th>recurrent_activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>recurrent_dropout</th>\n",
       "      <th>kernel_regularizer</th>\n",
       "      <th>recurrent_regularizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>l1l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>l1l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>l1l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1l2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1l2</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1l2</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1l2</td>\n",
       "      <td>l1l2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num activation recurrent_activation  dropout  recurrent_dropout  \\\n",
       "0     0    sigmoid                 tanh      0.2                0.1   \n",
       "1     1    sigmoid                 tanh      0.2                0.1   \n",
       "2     2    sigmoid                 tanh      0.2                0.1   \n",
       "3     3    sigmoid                 tanh      0.2                0.1   \n",
       "4     4    sigmoid                 tanh      0.2                0.1   \n",
       "5     5    sigmoid                 tanh      0.2                0.1   \n",
       "6     6    sigmoid                 tanh      0.2                0.1   \n",
       "7     7    sigmoid                 tanh      0.2                0.1   \n",
       "8     8    sigmoid                 tanh      0.2                0.1   \n",
       "9     9    sigmoid                 tanh      0.2                0.1   \n",
       "10   10    sigmoid                 tanh      0.2                0.1   \n",
       "11   11    sigmoid                 tanh      0.2                0.1   \n",
       "12   12    sigmoid                 tanh      0.2                0.1   \n",
       "13   13    sigmoid                 tanh      0.2                0.1   \n",
       "14   14    sigmoid                 tanh      0.2                0.1   \n",
       "15   15    sigmoid                 tanh      0.2                0.1   \n",
       "\n",
       "   kernel_regularizer recurrent_regularizer  \n",
       "0                None                  None  \n",
       "1                None                    l1  \n",
       "2                None                    l2  \n",
       "3                None                  l1l2  \n",
       "4                  l1                  None  \n",
       "5                  l1                    l1  \n",
       "6                  l1                    l2  \n",
       "7                  l1                  l1l2  \n",
       "8                  l2                  None  \n",
       "9                  l2                    l1  \n",
       "10                 l2                    l2  \n",
       "11                 l2                  l1l2  \n",
       "12               l1l2                  None  \n",
       "13               l1l2                    l1  \n",
       "14               l1l2                    l2  \n",
       "15               l1l2                  l1l2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regs = [ None, 'l1', 'l2', 'l1l2' ]\n",
    "\n",
    "activation, recurrent_activation, dropout, recurrent_dropout = 'sigmoid', 'tanh', 0.2, 0.1\n",
    "cases = []\n",
    "num = 0\n",
    "for kernel_regularizer in regs:\n",
    "    for recurrent_regularizer in regs:\n",
    "        cases.append((num, activation, recurrent_activation, dropout, recurrent_dropout, kernel_regularizer, recurrent_regularizer))\n",
    "        num += 1\n",
    "\n",
    "cases_df = pd.DataFrame(cases, columns=['num', 'activation', 'recurrent_activation', 'dropout', 'recurrent_dropout', 'kernel_regularizer', 'recurrent_regularizer'])\n",
    "cases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 sigmoid tanh 0.2 0.1 None l1l2\n",
      "Epoch 1/100\n",
      "2293/2293 - 47s - loss: 6352.6992 - mae: 53.3653 - mape: 355.3243 - mse: 6352.6992 - val_loss: 4800.0298 - val_mae: 48.1181 - val_mape: 537.9233 - val_mse: 4800.0298 - 47s/epoch - 21ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 45s - loss: 4625.4863 - mae: 46.3462 - mape: 402.6004 - mse: 4625.4863 - val_loss: 3760.1484 - val_mae: 40.4459 - val_mape: 341.5817 - val_mse: 3760.1484 - 45s/epoch - 20ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 47s - loss: 3909.1816 - mae: 44.2188 - mape: 333.5950 - mse: 3909.1816 - val_loss: 3504.1907 - val_mae: 39.5792 - val_mape: 330.5952 - val_mse: 3504.1907 - 47s/epoch - 21ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 47s - loss: 3672.2427 - mae: 43.4450 - mape: 321.5863 - mse: 3672.2427 - val_loss: 3355.1084 - val_mae: 39.3470 - val_mape: 336.6378 - val_mse: 3355.1084 - 47s/epoch - 20ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 47s - loss: 3540.8535 - mae: 42.2524 - mape: 307.3528 - mse: 3540.8535 - val_loss: 3244.4353 - val_mae: 37.0190 - val_mape: 284.3622 - val_mse: 3244.4353 - 47s/epoch - 20ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 47s - loss: 3358.5098 - mae: 41.2678 - mape: 294.8364 - mse: 3358.5098 - val_loss: 2958.2390 - val_mae: 35.0562 - val_mape: 259.3947 - val_mse: 2958.2390 - 47s/epoch - 20ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 47s - loss: 3077.4243 - mae: 39.1800 - mape: 269.0309 - mse: 3077.4243 - val_loss: 2981.1138 - val_mae: 34.3339 - val_mape: 175.3669 - val_mse: 2981.1138 - 47s/epoch - 20ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 46s - loss: 2852.4414 - mae: 37.1451 - mape: 239.7368 - mse: 2852.4414 - val_loss: 2664.9387 - val_mae: 32.9094 - val_mape: 201.7565 - val_mse: 2664.9387 - 46s/epoch - 20ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 46s - loss: 2669.1523 - mae: 35.6707 - mape: 229.8439 - mse: 2669.1523 - val_loss: 2535.6638 - val_mae: 31.9422 - val_mape: 200.7237 - val_mse: 2535.6638 - 46s/epoch - 20ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 46s - loss: 2517.3835 - mae: 34.2716 - mape: 222.2265 - mse: 2517.3835 - val_loss: 2896.4800 - val_mae: 34.6563 - val_mape: 220.5140 - val_mse: 2896.4800 - 46s/epoch - 20ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 46s - loss: 2402.3257 - mae: 33.2711 - mape: 205.6950 - mse: 2402.3257 - val_loss: 2569.2949 - val_mae: 33.1674 - val_mape: 258.9332 - val_mse: 2569.2949 - 46s/epoch - 20ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 46s - loss: 2334.4116 - mae: 32.5541 - mape: 195.6780 - mse: 2334.4116 - val_loss: 2043.1154 - val_mae: 29.2320 - val_mape: 215.0241 - val_mse: 2043.1154 - 46s/epoch - 20ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 46s - loss: 2301.9343 - mae: 32.6111 - mape: 189.3708 - mse: 2301.9343 - val_loss: 1938.1398 - val_mae: 28.4241 - val_mape: 215.7986 - val_mse: 1938.1398 - 46s/epoch - 20ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 45s - loss: 2217.6826 - mae: 31.8515 - mape: 182.8564 - mse: 2217.6826 - val_loss: 1856.9497 - val_mae: 28.0419 - val_mape: 223.4629 - val_mse: 1856.9497 - 45s/epoch - 20ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 46s - loss: 2168.9446 - mae: 31.5688 - mape: 183.3537 - mse: 2168.9446 - val_loss: 1946.6942 - val_mae: 28.6172 - val_mape: 215.8177 - val_mse: 1946.6942 - 46s/epoch - 20ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 46s - loss: 2176.3108 - mae: 31.7158 - mape: 191.5604 - mse: 2176.3108 - val_loss: 2098.5171 - val_mae: 29.4197 - val_mape: 214.8778 - val_mse: 2098.5171 - 46s/epoch - 20ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 45s - loss: 2122.3469 - mae: 31.1100 - mape: 189.3114 - mse: 2122.3469 - val_loss: 2024.3575 - val_mae: 29.1969 - val_mape: 214.3324 - val_mse: 2024.3575 - 45s/epoch - 20ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 46s - loss: 2069.5945 - mae: 30.6640 - mape: 182.1649 - mse: 2069.5945 - val_loss: 1980.6436 - val_mae: 29.0147 - val_mape: 244.8346 - val_mse: 1980.6436 - 46s/epoch - 20ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 45s - loss: 2034.1338 - mae: 30.3699 - mape: 183.8701 - mse: 2034.1338 - val_loss: 2010.1833 - val_mae: 28.9397 - val_mape: 221.3740 - val_mse: 2010.1833 - 45s/epoch - 20ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 45s - loss: 2017.9529 - mae: 30.0425 - mape: 176.9354 - mse: 2017.9529 - val_loss: 2002.6758 - val_mae: 28.9464 - val_mape: 251.2471 - val_mse: 2002.6758 - 45s/epoch - 20ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 46s - loss: 1999.6558 - mae: 30.0019 - mape: 181.3676 - mse: 1999.6558 - val_loss: 2043.4606 - val_mae: 29.1693 - val_mape: 236.2754 - val_mse: 2043.4606 - 46s/epoch - 20ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 45s - loss: 1986.2499 - mae: 29.9041 - mape: 182.3663 - mse: 1986.2499 - val_loss: 2070.8237 - val_mae: 29.3197 - val_mape: 243.0046 - val_mse: 2070.8237 - 45s/epoch - 20ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 46s - loss: 1976.1005 - mae: 29.8851 - mape: 180.8892 - mse: 1976.1005 - val_loss: 2022.1947 - val_mae: 28.7395 - val_mape: 234.0438 - val_mse: 2022.1947 - 46s/epoch - 20ms/step\n",
      "Epoch 24/100\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "2293/2293 - 45s - loss: 1930.9840 - mae: 29.5192 - mape: 176.6230 - mse: 1930.9840 - val_loss: 2015.6169 - val_mae: 28.6387 - val_mape: 225.0238 - val_mse: 2015.6169 - 45s/epoch - 20ms/step\n",
      "Epoch 24: early stopping\n",
      "727/727 [==============================] - 4s 6ms/step - loss: 2699.2239 - mae: 34.7494 - mape: 280.9170 - mse: 2699.2239\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-3/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-3/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023C39669EB0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 sigmoid tanh 0.2 0.1 l1 None\n",
      "Epoch 1/100\n",
      "2293/2293 - 51s - loss: 6293.8174 - mae: 53.1690 - mape: 366.2275 - mse: 6290.2217 - val_loss: 4831.0396 - val_mae: 48.8764 - val_mape: 575.1016 - val_mse: 4828.8574 - 51s/epoch - 22ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 47s - loss: 4681.6665 - mae: 47.0888 - mape: 428.3603 - mse: 4675.6338 - val_loss: 3822.1606 - val_mae: 41.7916 - val_mape: 449.3416 - val_mse: 3812.6868 - 47s/epoch - 21ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 47s - loss: 3912.9548 - mae: 44.1799 - mape: 342.5746 - mse: 3903.9951 - val_loss: 3454.3821 - val_mae: 39.5900 - val_mape: 367.8430 - val_mse: 3444.7910 - 47s/epoch - 20ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 46s - loss: 3686.5432 - mae: 43.6209 - mape: 322.9709 - mse: 3676.5347 - val_loss: 3280.1506 - val_mae: 38.3898 - val_mape: 332.8085 - val_mse: 3268.9526 - 46s/epoch - 20ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 46s - loss: 3524.0366 - mae: 42.4278 - mape: 299.0812 - mse: 3512.2156 - val_loss: 3185.8381 - val_mae: 35.3577 - val_mape: 204.8927 - val_mse: 3172.7698 - 46s/epoch - 20ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 46s - loss: 3237.3945 - mae: 40.3949 - mape: 262.7368 - mse: 3224.2288 - val_loss: 2965.7239 - val_mae: 34.2055 - val_mape: 204.2809 - val_mse: 2951.5791 - 46s/epoch - 20ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 47s - loss: 3022.6658 - mae: 38.5129 - mape: 246.9594 - mse: 3008.3877 - val_loss: 2821.1062 - val_mae: 33.5942 - val_mape: 210.8848 - val_mse: 2805.9871 - 47s/epoch - 20ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 46s - loss: 2843.8174 - mae: 37.1239 - mape: 244.1539 - mse: 2828.4421 - val_loss: 2646.7239 - val_mae: 32.6599 - val_mape: 216.9817 - val_mse: 2630.5476 - 46s/epoch - 20ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 47s - loss: 2755.0898 - mae: 36.3354 - mape: 237.9281 - mse: 2738.7112 - val_loss: 2568.1763 - val_mae: 31.9870 - val_mape: 196.8606 - val_mse: 2551.0706 - 47s/epoch - 20ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 46s - loss: 2646.7844 - mae: 35.2852 - mape: 224.5586 - mse: 2629.4460 - val_loss: 2434.1853 - val_mae: 31.1415 - val_mape: 199.3987 - val_mse: 2416.1816 - 46s/epoch - 20ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 46s - loss: 2545.7903 - mae: 34.5011 - mape: 223.8927 - mse: 2527.5679 - val_loss: 2369.5454 - val_mae: 30.9719 - val_mape: 224.9729 - val_mse: 2350.5286 - 46s/epoch - 20ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 46s - loss: 2488.0283 - mae: 34.0389 - mape: 213.0885 - mse: 2468.7910 - val_loss: 2350.6274 - val_mae: 30.6634 - val_mape: 211.9051 - val_mse: 2330.6919 - 46s/epoch - 20ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 46s - loss: 2434.4094 - mae: 33.6418 - mape: 211.3173 - mse: 2414.2271 - val_loss: 2256.0303 - val_mae: 29.9425 - val_mape: 208.4768 - val_mse: 2235.1602 - 46s/epoch - 20ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 46s - loss: 2377.4702 - mae: 33.2138 - mape: 203.3487 - mse: 2356.3506 - val_loss: 2219.2432 - val_mae: 29.7892 - val_mape: 206.5700 - val_mse: 2197.4897 - 46s/epoch - 20ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 46s - loss: 2343.0942 - mae: 32.8558 - mape: 198.8105 - mse: 2321.0886 - val_loss: 2189.0808 - val_mae: 29.3667 - val_mape: 208.3402 - val_mse: 2166.3660 - 46s/epoch - 20ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 46s - loss: 2285.9868 - mae: 32.4180 - mape: 197.1063 - mse: 2263.2395 - val_loss: 2141.2136 - val_mae: 29.1169 - val_mape: 203.2125 - val_mse: 2117.9011 - 46s/epoch - 20ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 46s - loss: 2255.0627 - mae: 32.0692 - mape: 189.5216 - mse: 2231.6382 - val_loss: 2128.2827 - val_mae: 29.0117 - val_mape: 209.9691 - val_mse: 2104.2893 - 46s/epoch - 20ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 46s - loss: 2218.3706 - mae: 31.8071 - mape: 188.9703 - mse: 2194.3645 - val_loss: 2169.5908 - val_mae: 29.5317 - val_mape: 215.6624 - val_mse: 2145.1011 - 46s/epoch - 20ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 46s - loss: 2195.2134 - mae: 31.6077 - mape: 188.1722 - mse: 2170.7742 - val_loss: 2189.9641 - val_mae: 29.7251 - val_mape: 220.5015 - val_mse: 2165.1035 - 46s/epoch - 20ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 46s - loss: 2163.7871 - mae: 31.3591 - mape: 187.7186 - mse: 2138.9214 - val_loss: 2223.5222 - val_mae: 30.0382 - val_mape: 228.3951 - val_mse: 2198.1379 - 46s/epoch - 20ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 46s - loss: 2154.1765 - mae: 31.1624 - mape: 183.7650 - mse: 2128.8894 - val_loss: 2202.0347 - val_mae: 29.8231 - val_mape: 229.2554 - val_mse: 2176.3599 - 46s/epoch - 20ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 46s - loss: 2127.4766 - mae: 30.9425 - mape: 184.0732 - mse: 2101.8062 - val_loss: 2196.3640 - val_mae: 29.7132 - val_mape: 222.6150 - val_mse: 2170.2466 - 46s/epoch - 20ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 46s - loss: 2092.3496 - mae: 30.5240 - mape: 178.6928 - mse: 2066.2520 - val_loss: 2241.9041 - val_mae: 29.8159 - val_mape: 208.9405 - val_mse: 2215.3232 - 46s/epoch - 20ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 46s - loss: 2081.9946 - mae: 30.3835 - mape: 182.2913 - mse: 2055.5137 - val_loss: 2206.6096 - val_mae: 29.3584 - val_mape: 198.2807 - val_mse: 2179.6978 - 46s/epoch - 20ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 46s - loss: 2031.7924 - mae: 30.0194 - mape: 181.0398 - mse: 2005.0542 - val_loss: 2218.7173 - val_mae: 29.3132 - val_mape: 186.8529 - val_mse: 2191.5786 - 46s/epoch - 20ms/step\n",
      "Epoch 26/100\n",
      "2293/2293 - 46s - loss: 1994.6692 - mae: 29.7140 - mape: 178.1669 - mse: 1967.6428 - val_loss: 2188.9780 - val_mae: 29.0041 - val_mape: 181.4840 - val_mse: 2161.4436 - 46s/epoch - 20ms/step\n",
      "Epoch 27/100\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "2293/2293 - 46s - loss: 1969.4128 - mae: 29.5037 - mape: 175.3369 - mse: 1942.0242 - val_loss: 2189.6394 - val_mae: 29.2183 - val_mape: 191.9665 - val_mse: 2161.7051 - 46s/epoch - 20ms/step\n",
      "Epoch 27: early stopping\n",
      "727/727 [==============================] - 4s 6ms/step - loss: 2674.5288 - mae: 34.4554 - mape: 285.0496 - mse: 2650.5369\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-4/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-4/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023C3B0AE3A0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 sigmoid tanh 0.2 0.1 l1 l1\n",
      "Epoch 1/100\n",
      "2293/2293 - 48s - loss: 6302.0439 - mae: 53.1229 - mape: 358.0248 - mse: 6294.2910 - val_loss: 4829.1890 - val_mae: 48.4940 - val_mape: 551.3431 - val_mse: 4820.9336 - 48s/epoch - 21ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 46s - loss: 4578.0825 - mae: 45.9988 - mape: 388.9713 - mse: 4566.9023 - val_loss: 4166.3994 - val_mae: 43.2956 - val_mape: 387.1135 - val_mse: 4153.7119 - 46s/epoch - 20ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 46s - loss: 3851.0286 - mae: 43.7562 - mape: 365.3172 - mse: 3837.8499 - val_loss: 5213.2700 - val_mae: 47.6724 - val_mape: 305.8122 - val_mse: 5198.8760 - 46s/epoch - 20ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 46s - loss: 3787.6348 - mae: 44.1134 - mape: 333.6133 - mse: 3772.5725 - val_loss: 3610.4836 - val_mae: 39.6698 - val_mape: 324.7452 - val_mse: 3594.4426 - 46s/epoch - 20ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 46s - loss: 3633.0659 - mae: 42.9778 - mape: 327.9717 - mse: 3616.0732 - val_loss: 4190.3955 - val_mae: 41.6197 - val_mape: 206.2278 - val_mse: 4171.8350 - 46s/epoch - 20ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 46s - loss: 3443.9187 - mae: 41.8432 - mape: 304.8256 - mse: 3425.1792 - val_loss: 6123.9683 - val_mae: 50.6651 - val_mape: 173.7212 - val_mse: 6103.9761 - 46s/epoch - 20ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 46s - loss: 3256.9858 - mae: 40.4022 - mape: 280.2984 - mse: 3236.4282 - val_loss: 4144.6797 - val_mae: 40.3510 - val_mape: 202.2073 - val_mse: 4123.1572 - 46s/epoch - 20ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 46s - loss: 3022.6514 - mae: 38.4688 - mape: 250.7226 - mse: 3000.9043 - val_loss: 2852.0161 - val_mae: 33.7387 - val_mape: 214.1382 - val_mse: 2829.2415 - 46s/epoch - 20ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 46s - loss: 2863.6360 - mae: 37.2962 - mape: 243.7229 - mse: 2840.6882 - val_loss: 2553.3188 - val_mae: 32.1789 - val_mape: 234.4672 - val_mse: 2529.6270 - 46s/epoch - 20ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 46s - loss: 2712.6501 - mae: 35.7866 - mape: 238.8580 - mse: 2688.6948 - val_loss: 2413.4827 - val_mae: 30.9788 - val_mape: 223.1416 - val_mse: 2388.6689 - 46s/epoch - 20ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 46s - loss: 2623.4868 - mae: 34.9040 - mape: 228.1275 - mse: 2598.0132 - val_loss: 2484.4204 - val_mae: 31.1108 - val_mape: 227.1793 - val_mse: 2457.8098 - 46s/epoch - 20ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 46s - loss: 2577.7681 - mae: 34.4369 - mape: 221.1179 - mse: 2550.5781 - val_loss: 2235.6729 - val_mae: 29.5867 - val_mape: 212.0632 - val_mse: 2207.4810 - 46s/epoch - 20ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 46s - loss: 2459.8208 - mae: 33.6008 - mape: 211.6890 - mse: 2431.2839 - val_loss: 2283.2617 - val_mae: 29.8214 - val_mape: 243.0159 - val_mse: 2253.9006 - 46s/epoch - 20ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 46s - loss: 2356.2544 - mae: 32.9600 - mape: 205.6880 - mse: 2326.5273 - val_loss: 2165.1074 - val_mae: 29.1129 - val_mape: 219.6892 - val_mse: 2134.4246 - 46s/epoch - 20ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 46s - loss: 2362.7864 - mae: 33.1052 - mape: 210.3972 - mse: 2331.5535 - val_loss: 2226.8782 - val_mae: 29.2305 - val_mape: 185.1733 - val_mse: 2194.4697 - 46s/epoch - 20ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 46s - loss: 2296.0559 - mae: 32.5170 - mape: 201.0023 - mse: 2263.5273 - val_loss: 2196.8062 - val_mae: 28.9416 - val_mape: 207.7506 - val_mse: 2163.6099 - 46s/epoch - 20ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 46s - loss: 2258.4143 - mae: 32.0701 - mape: 200.7759 - mse: 2224.7866 - val_loss: 2022.3473 - val_mae: 27.4946 - val_mape: 185.6071 - val_mse: 1987.7761 - 46s/epoch - 20ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 46s - loss: 2179.0437 - mae: 31.5920 - mape: 197.6673 - mse: 2144.3069 - val_loss: 2077.4497 - val_mae: 28.0279 - val_mape: 197.7535 - val_mse: 2042.0563 - 46s/epoch - 20ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 47s - loss: 2154.2695 - mae: 31.3537 - mape: 192.8806 - mse: 2118.8406 - val_loss: 2201.9146 - val_mae: 29.0145 - val_mape: 198.0295 - val_mse: 2165.8311 - 47s/epoch - 20ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 47s - loss: 2110.2605 - mae: 30.8894 - mape: 192.2939 - mse: 2073.9937 - val_loss: 2260.2783 - val_mae: 29.6443 - val_mape: 210.5298 - val_mse: 2223.3445 - 47s/epoch - 20ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 47s - loss: 2068.0247 - mae: 30.4868 - mape: 187.3408 - mse: 2030.9751 - val_loss: 2141.5730 - val_mae: 29.0692 - val_mape: 188.1762 - val_mse: 2104.0547 - 47s/epoch - 20ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 47s - loss: 2076.8750 - mae: 30.5386 - mape: 189.5738 - mse: 2039.1959 - val_loss: 2635.2878 - val_mae: 31.9469 - val_mape: 213.3043 - val_mse: 2596.8242 - 47s/epoch - 20ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 47s - loss: 2066.4512 - mae: 30.5420 - mape: 196.0536 - mse: 2027.9301 - val_loss: 2309.3267 - val_mae: 29.6830 - val_mape: 197.5107 - val_mse: 2270.2695 - 47s/epoch - 20ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 47s - loss: 2034.1985 - mae: 30.0654 - mape: 189.6985 - mse: 1994.8567 - val_loss: 2434.1841 - val_mae: 30.8755 - val_mape: 216.1720 - val_mse: 2394.1089 - 47s/epoch - 20ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 47s - loss: 1998.0154 - mae: 29.7835 - mape: 186.3004 - mse: 1958.1021 - val_loss: 2443.0381 - val_mae: 30.8886 - val_mape: 194.2355 - val_mse: 2402.7603 - 47s/epoch - 21ms/step\n",
      "Epoch 26/100\n",
      "2293/2293 - 47s - loss: 1947.9785 - mae: 29.2100 - mape: 181.9638 - mse: 1907.6794 - val_loss: 2417.4287 - val_mae: 30.3910 - val_mape: 197.1408 - val_mse: 2376.7100 - 47s/epoch - 21ms/step\n",
      "Epoch 27/100\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "2293/2293 - 48s - loss: 1914.6620 - mae: 28.9056 - mape: 176.2491 - mse: 1873.8390 - val_loss: 2412.5735 - val_mae: 30.3079 - val_mape: 184.7476 - val_mse: 2371.3257 - 48s/epoch - 21ms/step\n",
      "Epoch 27: early stopping\n",
      "727/727 [==============================] - 4s 6ms/step - loss: 2549.0647 - mae: 32.3894 - mape: 246.1999 - mse: 2514.4883\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-5/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-5/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023B84591A00> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 sigmoid tanh 0.2 0.1 l1 l2\n",
      "Epoch 1/100\n",
      "2293/2293 - 75s - loss: 6284.2266 - mae: 53.0631 - mape: 363.7915 - mse: 6279.1792 - val_loss: 4800.6802 - val_mae: 48.5643 - val_mape: 565.2667 - val_mse: 4796.0635 - 75s/epoch - 33ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 71s - loss: 4681.4038 - mae: 48.1759 - mape: 517.9295 - mse: 4674.2339 - val_loss: 3854.5103 - val_mae: 41.4200 - val_mape: 404.0339 - val_mse: 3844.8777 - 71s/epoch - 31ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 74s - loss: 3929.3811 - mae: 44.5520 - mape: 380.4067 - mse: 3918.9543 - val_loss: 3631.7192 - val_mae: 40.5655 - val_mape: 379.5464 - val_mse: 3620.6536 - 74s/epoch - 32ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 74s - loss: 3757.7578 - mae: 44.0108 - mape: 345.9918 - mse: 3745.9092 - val_loss: 3397.1614 - val_mae: 39.4374 - val_mape: 341.3830 - val_mse: 3384.0828 - 74s/epoch - 32ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 72s - loss: 3602.7151 - mae: 43.0993 - mape: 335.9199 - mse: 3588.9229 - val_loss: 3256.5840 - val_mae: 38.1256 - val_mape: 325.3956 - val_mse: 3241.6978 - 72s/epoch - 32ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 74s - loss: 3443.5979 - mae: 41.9121 - mape: 304.2871 - mse: 3428.0583 - val_loss: 3005.1228 - val_mae: 35.5378 - val_mape: 254.6270 - val_mse: 2988.7434 - 74s/epoch - 32ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 75s - loss: 3264.7800 - mae: 40.4426 - mape: 281.4322 - mse: 3247.7048 - val_loss: 3177.0125 - val_mae: 35.0680 - val_mape: 194.9622 - val_mse: 3159.2617 - 75s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 78s - loss: 2968.3882 - mae: 38.3410 - mape: 257.0743 - mse: 2949.8315 - val_loss: 3130.6016 - val_mae: 35.2743 - val_mape: 208.0978 - val_mse: 3110.9812 - 78s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 74s - loss: 2872.2585 - mae: 37.2551 - mape: 243.2135 - mse: 2852.1501 - val_loss: 2689.5830 - val_mae: 33.1372 - val_mape: 208.1722 - val_mse: 2668.4985 - 74s/epoch - 32ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 77s - loss: 2707.7390 - mae: 36.1157 - mape: 238.7300 - mse: 2686.2605 - val_loss: 2486.2334 - val_mae: 32.5451 - val_mape: 235.8656 - val_mse: 2464.2175 - 77s/epoch - 34ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 73s - loss: 2600.1326 - mae: 35.1203 - mape: 235.0527 - mse: 2577.9561 - val_loss: 2371.8618 - val_mae: 31.0934 - val_mape: 207.8973 - val_mse: 2348.9170 - 73s/epoch - 32ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 71s - loss: 2459.1562 - mae: 33.3279 - mape: 207.0116 - mse: 2435.8640 - val_loss: 2288.5693 - val_mae: 30.6287 - val_mape: 242.3493 - val_mse: 2264.5557 - 71s/epoch - 31ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 71s - loss: 2402.2195 - mae: 33.1157 - mape: 203.8559 - mse: 2378.0012 - val_loss: 2209.9167 - val_mae: 29.6736 - val_mape: 216.1239 - val_mse: 2184.9741 - 71s/epoch - 31ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 71s - loss: 2306.6282 - mae: 32.2226 - mape: 205.0352 - mse: 2281.2126 - val_loss: 2279.4189 - val_mae: 30.9080 - val_mape: 211.4447 - val_mse: 2253.2861 - 71s/epoch - 31ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 73s - loss: 2279.2307 - mae: 32.2172 - mape: 198.4249 - mse: 2252.7146 - val_loss: 2119.6609 - val_mae: 29.6518 - val_mape: 189.9016 - val_mse: 2092.4492 - 73s/epoch - 32ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 75s - loss: 2230.1150 - mae: 31.8393 - mape: 194.2178 - mse: 2202.6079 - val_loss: 2135.4016 - val_mae: 29.8549 - val_mape: 200.6487 - val_mse: 2107.1387 - 75s/epoch - 33ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 73s - loss: 2195.4810 - mae: 31.4903 - mape: 191.2657 - mse: 2166.7485 - val_loss: 2097.0359 - val_mae: 29.9063 - val_mape: 222.3419 - val_mse: 2067.6431 - 73s/epoch - 32ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 71s - loss: 2186.6931 - mae: 31.5248 - mape: 194.3746 - mse: 2156.7485 - val_loss: 2066.1675 - val_mae: 28.9670 - val_mape: 192.0088 - val_mse: 2035.5286 - 71s/epoch - 31ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 71s - loss: 2124.1021 - mae: 31.0094 - mape: 193.2918 - mse: 2093.1118 - val_loss: 2042.8610 - val_mae: 28.8002 - val_mape: 192.9126 - val_mse: 2011.3706 - 71s/epoch - 31ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 73s - loss: 2120.5603 - mae: 30.8981 - mape: 193.2728 - mse: 2088.5378 - val_loss: 2026.7384 - val_mae: 28.4200 - val_mape: 178.2580 - val_mse: 1994.1987 - 73s/epoch - 32ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 71s - loss: 2074.1790 - mae: 30.5185 - mape: 187.5298 - mse: 2041.2577 - val_loss: 2006.0271 - val_mae: 28.1054 - val_mape: 192.9891 - val_mse: 1972.5597 - 71s/epoch - 31ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 73s - loss: 2063.5476 - mae: 30.4522 - mape: 190.8144 - mse: 2029.6410 - val_loss: 2090.8318 - val_mae: 28.4853 - val_mape: 194.6562 - val_mse: 2056.3726 - 73s/epoch - 32ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 71s - loss: 2042.8813 - mae: 30.3045 - mape: 186.0806 - mse: 2008.0831 - val_loss: 1974.3713 - val_mae: 28.1152 - val_mape: 207.7350 - val_mse: 1938.7795 - 71s/epoch - 31ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 71s - loss: 2011.8508 - mae: 30.0572 - mape: 187.5663 - mse: 1976.0988 - val_loss: 2066.8672 - val_mae: 28.4122 - val_mape: 194.8963 - val_mse: 2030.5410 - 71s/epoch - 31ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 70s - loss: 1996.7990 - mae: 29.9353 - mape: 185.6935 - mse: 1960.1296 - val_loss: 2084.1433 - val_mae: 28.9017 - val_mape: 194.8791 - val_mse: 2046.9874 - 70s/epoch - 31ms/step\n",
      "Epoch 26/100\n",
      "2293/2293 - 70s - loss: 1980.3147 - mae: 29.7027 - mape: 188.8963 - mse: 1942.7704 - val_loss: 2151.2625 - val_mae: 29.1919 - val_mape: 196.4673 - val_mse: 2113.2188 - 70s/epoch - 31ms/step\n",
      "Epoch 27/100\n",
      "2293/2293 - 70s - loss: 1977.4359 - mae: 29.6013 - mape: 183.9344 - mse: 1939.0988 - val_loss: 2167.4707 - val_mae: 29.3366 - val_mape: 202.3220 - val_mse: 2128.5725 - 70s/epoch - 30ms/step\n",
      "Epoch 28/100\n",
      "2293/2293 - 70s - loss: 1973.8055 - mae: 29.5430 - mape: 183.6180 - mse: 1934.6572 - val_loss: 2191.0505 - val_mae: 29.5487 - val_mape: 213.6095 - val_mse: 2151.3228 - 70s/epoch - 31ms/step\n",
      "Epoch 29/100\n",
      "2293/2293 - 74s - loss: 1961.0793 - mae: 29.4097 - mape: 182.0017 - mse: 1921.0693 - val_loss: 2155.2007 - val_mae: 29.4956 - val_mape: 230.5251 - val_mse: 2114.5396 - 74s/epoch - 32ms/step\n",
      "Epoch 30/100\n",
      "2293/2293 - 72s - loss: 1933.5844 - mae: 29.1333 - mape: 182.6591 - mse: 1892.8301 - val_loss: 2235.8848 - val_mae: 29.7914 - val_mape: 220.5312 - val_mse: 2194.6826 - 72s/epoch - 31ms/step\n",
      "Epoch 31/100\n",
      "2293/2293 - 76s - loss: 1923.5070 - mae: 28.9706 - mape: 183.5319 - mse: 1882.1259 - val_loss: 2200.0623 - val_mae: 29.4355 - val_mape: 214.6881 - val_mse: 2158.2556 - 76s/epoch - 33ms/step\n",
      "Epoch 32/100\n",
      "2293/2293 - 72s - loss: 1917.9692 - mae: 28.8475 - mape: 179.5527 - mse: 1875.8711 - val_loss: 2214.6956 - val_mae: 29.4154 - val_mape: 219.5721 - val_mse: 2172.1174 - 72s/epoch - 32ms/step\n",
      "Epoch 33/100\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "2293/2293 - 71s - loss: 1924.2736 - mae: 28.8394 - mape: 180.2760 - mse: 1881.4413 - val_loss: 2270.6965 - val_mae: 29.2726 - val_mape: 203.5804 - val_mse: 2227.4417 - 71s/epoch - 31ms/step\n",
      "Epoch 33: early stopping\n",
      "727/727 [==============================] - 8s 11ms/step - loss: 2796.3979 - mae: 34.6099 - mape: 277.7648 - mse: 2760.8101\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-6/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-6/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023BC98BAE80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 sigmoid tanh 0.2 0.1 l1 l1l2\n",
      "Epoch 1/100\n",
      "2293/2293 - 66s - loss: 6326.4961 - mae: 53.2743 - mape: 361.7276 - mse: 6323.0894 - val_loss: 4841.8965 - val_mae: 48.8681 - val_mape: 571.5199 - val_mse: 4839.8770 - 66s/epoch - 29ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 61s - loss: 4650.4463 - mae: 47.0223 - mape: 431.3628 - mse: 4645.2949 - val_loss: 3867.4441 - val_mae: 40.8713 - val_mape: 329.6993 - val_mse: 3861.2881 - 61s/epoch - 26ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 62s - loss: 4003.2554 - mae: 44.9526 - mape: 383.0060 - mse: 3996.1423 - val_loss: 3702.2468 - val_mae: 40.0341 - val_mape: 326.5708 - val_mse: 3693.7649 - 62s/epoch - 27ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 61s - loss: 3731.4119 - mae: 43.9897 - mape: 323.4497 - mse: 3722.6829 - val_loss: 3288.5620 - val_mae: 38.4824 - val_mape: 345.1718 - val_mse: 3278.1743 - 61s/epoch - 26ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 64s - loss: 3545.9141 - mae: 42.5796 - mape: 302.6289 - mse: 3534.9360 - val_loss: 3127.8989 - val_mae: 36.6101 - val_mape: 294.0273 - val_mse: 3115.5435 - 64s/epoch - 28ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 62s - loss: 3324.6758 - mae: 40.8514 - mape: 261.7554 - mse: 3312.0569 - val_loss: 3988.2119 - val_mae: 40.6007 - val_mape: 202.3343 - val_mse: 3974.7507 - 62s/epoch - 27ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 62s - loss: 3077.6409 - mae: 39.1031 - mape: 241.7565 - mse: 3063.9272 - val_loss: 2801.3015 - val_mae: 33.0433 - val_mape: 210.3189 - val_mse: 2786.7190 - 62s/epoch - 27ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 64s - loss: 2856.1025 - mae: 37.0945 - mape: 241.8632 - mse: 2841.2566 - val_loss: 2722.4863 - val_mae: 32.7259 - val_mape: 223.8787 - val_mse: 2706.8433 - 64s/epoch - 28ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 62s - loss: 2753.6394 - mae: 36.1339 - mape: 237.6627 - mse: 2737.8977 - val_loss: 2576.7649 - val_mae: 31.8852 - val_mape: 226.2670 - val_mse: 2560.1941 - 62s/epoch - 27ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 64s - loss: 2642.3828 - mae: 35.2932 - mape: 221.9073 - mse: 2625.8186 - val_loss: 2400.3010 - val_mae: 31.2206 - val_mape: 213.6053 - val_mse: 2383.0208 - 64s/epoch - 28ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 61s - loss: 2555.8838 - mae: 34.5045 - mape: 216.0239 - mse: 2538.5371 - val_loss: 2378.9412 - val_mae: 30.8466 - val_mape: 191.2997 - val_mse: 2361.0173 - 61s/epoch - 26ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 61s - loss: 2468.1169 - mae: 33.8439 - mape: 209.4230 - mse: 2449.9409 - val_loss: 2306.2922 - val_mae: 30.3529 - val_mape: 178.9453 - val_mse: 2287.5618 - 61s/epoch - 27ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 63s - loss: 2421.8992 - mae: 33.3568 - mape: 204.4952 - mse: 2402.9365 - val_loss: 2265.9653 - val_mae: 29.9130 - val_mape: 220.2903 - val_mse: 2246.3010 - 63s/epoch - 27ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 64s - loss: 2359.4087 - mae: 32.8415 - mape: 204.1733 - mse: 2339.6624 - val_loss: 2238.5984 - val_mae: 29.8084 - val_mape: 203.0774 - val_mse: 2218.1543 - 64s/epoch - 28ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 64s - loss: 2342.4072 - mae: 32.6597 - mape: 197.7424 - mse: 2321.8684 - val_loss: 2177.0728 - val_mae: 29.2930 - val_mape: 178.6691 - val_mse: 2155.8806 - 64s/epoch - 28ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 62s - loss: 2278.3220 - mae: 32.1844 - mape: 188.3535 - mse: 2257.0952 - val_loss: 2082.9575 - val_mae: 28.7063 - val_mape: 196.2435 - val_mse: 2061.0667 - 62s/epoch - 27ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 63s - loss: 2247.2991 - mae: 31.8986 - mape: 194.3696 - mse: 2225.2891 - val_loss: 2107.1321 - val_mae: 29.0775 - val_mape: 200.2211 - val_mse: 2084.6255 - 63s/epoch - 27ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 63s - loss: 2192.0879 - mae: 31.5507 - mape: 191.2596 - mse: 2169.5142 - val_loss: 2213.0876 - val_mae: 29.1829 - val_mape: 190.9760 - val_mse: 2190.0767 - 63s/epoch - 28ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 63s - loss: 2159.4629 - mae: 31.1820 - mape: 188.7512 - mse: 2136.3540 - val_loss: 2255.3464 - val_mae: 29.6416 - val_mape: 211.5124 - val_mse: 2231.7607 - 63s/epoch - 28ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 62s - loss: 2113.7871 - mae: 30.8579 - mape: 191.7782 - mse: 2090.2346 - val_loss: 2277.5574 - val_mae: 30.1325 - val_mape: 211.8836 - val_mse: 2253.4651 - 62s/epoch - 27ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 62s - loss: 2095.4832 - mae: 30.7107 - mape: 191.9695 - mse: 2071.3276 - val_loss: 2293.1042 - val_mae: 30.3842 - val_mape: 218.5057 - val_mse: 2268.4824 - 62s/epoch - 27ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 64s - loss: 2111.4302 - mae: 30.7750 - mape: 194.7814 - mse: 2086.7134 - val_loss: 2286.5945 - val_mae: 29.5712 - val_mape: 183.7080 - val_mse: 2261.3711 - 64s/epoch - 28ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 63s - loss: 2079.1091 - mae: 30.3171 - mape: 187.0430 - mse: 2053.8674 - val_loss: 2218.4768 - val_mae: 29.6728 - val_mape: 202.3459 - val_mse: 2192.7207 - 63s/epoch - 27ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 64s - loss: 2054.7131 - mae: 30.2480 - mape: 183.4846 - mse: 2028.9792 - val_loss: 2152.7502 - val_mae: 29.0822 - val_mape: 197.3659 - val_mse: 2126.6372 - 64s/epoch - 28ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 63s - loss: 2011.8602 - mae: 29.9425 - mape: 182.2464 - mse: 1985.7762 - val_loss: 2199.5176 - val_mae: 29.3408 - val_mape: 190.0042 - val_mse: 2172.9360 - 63s/epoch - 27ms/step\n",
      "Epoch 26/100\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "2293/2293 - 63s - loss: 2012.6036 - mae: 29.8718 - mape: 182.0283 - mse: 1986.0839 - val_loss: 2244.8860 - val_mae: 29.6281 - val_mape: 182.4944 - val_mse: 2217.8918 - 63s/epoch - 28ms/step\n",
      "Epoch 26: early stopping\n",
      "727/727 [==============================] - 7s 9ms/step - loss: 2705.7039 - mae: 34.1768 - mape: 249.1449 - mse: 2683.8152\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-7/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-7/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023B84751940> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 sigmoid tanh 0.2 0.1 l2 None\n",
      "Epoch 1/100\n",
      "2293/2293 - 92s - loss: 6286.2700 - mae: 53.1458 - mape: 366.8901 - mse: 6285.4473 - val_loss: 4827.9897 - val_mae: 48.8786 - val_mape: 575.6696 - val_mse: 4827.3071 - 92s/epoch - 40ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 83s - loss: 4600.4751 - mae: 45.9701 - mape: 380.2219 - mse: 4599.1270 - val_loss: 3813.9985 - val_mae: 41.1962 - val_mape: 415.0617 - val_mse: 3812.2522 - 83s/epoch - 36ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 83s - loss: 3896.3279 - mae: 44.2196 - mape: 345.5121 - mse: 3894.1936 - val_loss: 3674.6467 - val_mae: 40.2296 - val_mape: 335.8683 - val_mse: 3672.0056 - 83s/epoch - 36ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 85s - loss: 3749.0698 - mae: 43.7703 - mape: 342.2764 - mse: 3746.1741 - val_loss: 3400.6926 - val_mae: 39.9006 - val_mape: 380.5186 - val_mse: 3397.1243 - 85s/epoch - 37ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 82s - loss: 3560.0435 - mae: 42.6429 - mape: 324.4381 - mse: 3556.0713 - val_loss: 3249.6187 - val_mae: 36.6158 - val_mape: 253.6224 - val_mse: 3244.9004 - 82s/epoch - 36ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 80s - loss: 3314.0718 - mae: 40.8583 - mape: 278.4069 - mse: 3308.8279 - val_loss: 3086.3728 - val_mae: 34.8692 - val_mape: 212.4194 - val_mse: 3080.5391 - 80s/epoch - 35ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 80s - loss: 3076.3577 - mae: 39.1942 - mape: 249.2124 - mse: 3069.9094 - val_loss: 2905.6326 - val_mae: 34.0881 - val_mape: 213.0515 - val_mse: 2898.4065 - 80s/epoch - 35ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 79s - loss: 2946.1831 - mae: 37.7616 - mape: 247.3999 - mse: 2938.4392 - val_loss: 2776.0447 - val_mae: 33.5637 - val_mape: 198.0031 - val_mse: 2767.6616 - 79s/epoch - 35ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 80s - loss: 2787.7476 - mae: 36.4690 - mape: 231.2679 - mse: 2778.8923 - val_loss: 2558.9797 - val_mae: 32.2320 - val_mape: 237.5072 - val_mse: 2549.4236 - 80s/epoch - 35ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 79s - loss: 2656.3435 - mae: 35.4274 - mape: 217.5720 - mse: 2646.3313 - val_loss: 2472.9761 - val_mae: 31.6654 - val_mape: 235.0240 - val_mse: 2462.2900 - 79s/epoch - 35ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 79s - loss: 2566.7146 - mae: 34.6559 - mape: 215.7254 - mse: 2555.4663 - val_loss: 2453.8206 - val_mae: 31.2046 - val_mape: 216.5094 - val_mse: 2441.9155 - 79s/epoch - 35ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 80s - loss: 2502.2307 - mae: 34.2176 - mape: 208.9613 - mse: 2489.7524 - val_loss: 2467.6826 - val_mae: 31.0158 - val_mape: 208.9903 - val_mse: 2454.5515 - 80s/epoch - 35ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 80s - loss: 2433.6602 - mae: 33.7468 - mape: 203.4866 - mse: 2419.8777 - val_loss: 2356.4661 - val_mae: 30.0298 - val_mape: 196.9919 - val_mse: 2341.9570 - 80s/epoch - 35ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 80s - loss: 2385.4768 - mae: 33.3886 - mape: 204.2173 - mse: 2370.3052 - val_loss: 2285.1340 - val_mae: 29.3623 - val_mape: 199.0262 - val_mse: 2269.3052 - 80s/epoch - 35ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 79s - loss: 2356.2883 - mae: 33.1942 - mape: 202.0508 - mse: 2339.8059 - val_loss: 2193.2434 - val_mae: 29.0300 - val_mape: 208.1894 - val_mse: 2176.0371 - 79s/epoch - 35ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 79s - loss: 2337.9241 - mae: 32.9147 - mape: 196.0025 - mse: 2320.1309 - val_loss: 2299.7400 - val_mae: 30.2510 - val_mape: 249.2716 - val_mse: 2281.2605 - 79s/epoch - 34ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 79s - loss: 2344.4453 - mae: 32.6999 - mape: 192.4158 - mse: 2325.5374 - val_loss: 2266.0640 - val_mae: 29.6139 - val_mape: 227.6056 - val_mse: 2246.5955 - 79s/epoch - 34ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 79s - loss: 2309.7349 - mae: 32.6265 - mape: 191.0713 - mse: 2289.7898 - val_loss: 2231.7310 - val_mae: 29.7429 - val_mape: 229.9773 - val_mse: 2211.2439 - 79s/epoch - 35ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 79s - loss: 2255.2937 - mae: 32.2010 - mape: 188.8793 - mse: 2234.3352 - val_loss: 2188.4016 - val_mae: 29.5001 - val_mape: 226.5571 - val_mse: 2166.9563 - 79s/epoch - 34ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 80s - loss: 2211.1521 - mae: 31.8002 - mape: 192.6788 - mse: 2189.2866 - val_loss: 2158.0776 - val_mae: 29.0775 - val_mape: 220.6865 - val_mse: 2135.7803 - 80s/epoch - 35ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 83s - loss: 2175.7324 - mae: 31.4909 - mape: 191.2629 - mse: 2153.0249 - val_loss: 2125.0728 - val_mae: 28.7006 - val_mape: 221.6579 - val_mse: 2101.9373 - 83s/epoch - 36ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 83s - loss: 2150.0264 - mae: 31.2632 - mape: 190.1503 - mse: 2126.4963 - val_loss: 2136.8906 - val_mae: 29.0824 - val_mape: 227.3944 - val_mse: 2113.0137 - 83s/epoch - 36ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 83s - loss: 2122.6646 - mae: 30.9941 - mape: 185.9762 - mse: 2098.4399 - val_loss: 2105.5435 - val_mae: 29.0040 - val_mape: 232.4382 - val_mse: 2081.0022 - 83s/epoch - 36ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 82s - loss: 2104.8425 - mae: 30.9052 - mape: 185.7890 - mse: 2079.9116 - val_loss: 2152.2739 - val_mae: 29.4650 - val_mape: 244.7128 - val_mse: 2127.0845 - 82s/epoch - 36ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 83s - loss: 2100.4226 - mae: 30.8831 - mape: 192.6552 - mse: 2074.8179 - val_loss: 2232.3652 - val_mae: 30.0700 - val_mape: 213.7619 - val_mse: 2206.4443 - 83s/epoch - 36ms/step\n",
      "Epoch 26/100\n",
      "2293/2293 - 83s - loss: 2047.1202 - mae: 30.3300 - mape: 180.2373 - mse: 2020.7498 - val_loss: 2214.6270 - val_mae: 29.7515 - val_mape: 227.1495 - val_mse: 2187.9082 - 83s/epoch - 36ms/step\n",
      "Epoch 27/100\n",
      "2293/2293 - 83s - loss: 2032.4379 - mae: 30.1857 - mape: 183.4980 - mse: 2005.3032 - val_loss: 2237.6758 - val_mae: 29.7121 - val_mape: 208.9117 - val_mse: 2210.2234 - 83s/epoch - 36ms/step\n",
      "Epoch 28/100\n",
      "2293/2293 - 85s - loss: 2011.0858 - mae: 30.0003 - mape: 183.4037 - mse: 1983.1776 - val_loss: 2223.2312 - val_mae: 29.8629 - val_mape: 194.3160 - val_mse: 2194.9319 - 85s/epoch - 37ms/step\n",
      "Epoch 29/100\n",
      "2293/2293 - 83s - loss: 1992.6517 - mae: 29.8001 - mape: 177.9235 - mse: 1963.9594 - val_loss: 2200.0276 - val_mae: 29.8229 - val_mape: 209.1143 - val_mse: 2170.9985 - 83s/epoch - 36ms/step\n",
      "Epoch 30/100\n",
      "2293/2293 - 83s - loss: 1986.0118 - mae: 29.6954 - mape: 179.2520 - mse: 1956.5758 - val_loss: 2272.0190 - val_mae: 29.9031 - val_mape: 204.6738 - val_mse: 2242.3196 - 83s/epoch - 36ms/step\n",
      "Epoch 31/100\n",
      "2293/2293 - 82s - loss: 1976.8075 - mae: 29.6248 - mape: 178.3241 - mse: 1946.7163 - val_loss: 2297.4131 - val_mae: 30.1381 - val_mape: 212.3999 - val_mse: 2267.0071 - 82s/epoch - 36ms/step\n",
      "Epoch 32/100\n",
      "2293/2293 - 84s - loss: 1963.2565 - mae: 29.5283 - mape: 183.6819 - mse: 1932.4166 - val_loss: 2316.9534 - val_mae: 30.3253 - val_mape: 210.1713 - val_mse: 2285.8447 - 84s/epoch - 37ms/step\n",
      "Epoch 33/100\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "2293/2293 - 83s - loss: 1970.3511 - mae: 29.5949 - mape: 184.8664 - mse: 1938.7781 - val_loss: 2271.4155 - val_mae: 29.8184 - val_mape: 207.1525 - val_mse: 2239.5632 - 83s/epoch - 36ms/step\n",
      "Epoch 33: early stopping\n",
      "727/727 [==============================] - 10s 13ms/step - loss: 2587.1182 - mae: 33.5542 - mape: 264.9724 - mse: 2562.5740\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-8/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-8/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023C3ADD52B0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 sigmoid tanh 0.2 0.1 l2 l1\n",
      "Epoch 1/100\n",
      "2293/2293 - 86s - loss: 6367.3291 - mae: 53.2407 - mape: 335.6918 - mse: 6362.9209 - val_loss: 4711.9341 - val_mae: 45.7369 - val_mape: 407.8567 - val_mse: 4706.1602 - 86s/epoch - 38ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 82s - loss: 4509.2485 - mae: 45.8033 - mape: 374.7807 - mse: 4501.6025 - val_loss: 3870.4885 - val_mae: 40.8594 - val_mape: 328.1596 - val_mse: 3861.8911 - 82s/epoch - 36ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 80s - loss: 3961.7363 - mae: 44.6996 - mape: 360.2111 - mse: 3952.4150 - val_loss: 3491.1511 - val_mae: 39.5449 - val_mape: 355.4040 - val_mse: 3480.5759 - 80s/epoch - 35ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 83s - loss: 3686.9316 - mae: 43.5230 - mape: 339.5639 - mse: 3675.2188 - val_loss: 3266.2556 - val_mae: 38.0267 - val_mape: 326.2221 - val_mse: 3253.3669 - 83s/epoch - 36ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 82s - loss: 3486.5405 - mae: 42.1041 - mape: 315.9851 - mse: 3472.6848 - val_loss: 3040.0935 - val_mae: 35.3041 - val_mape: 260.6112 - val_mse: 3025.0964 - 82s/epoch - 36ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 79s - loss: 3246.5745 - mae: 40.6277 - mape: 292.5872 - mse: 3230.6121 - val_loss: 2882.3486 - val_mae: 34.1857 - val_mape: 232.9308 - val_mse: 2865.2988 - 79s/epoch - 34ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 80s - loss: 3039.0103 - mae: 39.0256 - mape: 284.2204 - mse: 3020.8213 - val_loss: 2685.1252 - val_mae: 33.0996 - val_mape: 228.0109 - val_mse: 2665.9111 - 80s/epoch - 35ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 78s - loss: 2848.7964 - mae: 37.4278 - mape: 266.7173 - mse: 2828.5444 - val_loss: 2599.6848 - val_mae: 32.8709 - val_mape: 254.4656 - val_mse: 2578.3926 - 78s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 77s - loss: 2735.8049 - mae: 36.4414 - mape: 265.0912 - mse: 2713.4502 - val_loss: 2444.7085 - val_mae: 31.8066 - val_mape: 258.6597 - val_mse: 2421.2085 - 77s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 77s - loss: 2673.7585 - mae: 35.7024 - mape: 252.2090 - mse: 2649.2515 - val_loss: 2445.6626 - val_mae: 31.5155 - val_mape: 267.5886 - val_mse: 2419.8291 - 77s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 76s - loss: 2603.5337 - mae: 35.2515 - mape: 261.9605 - mse: 2576.9072 - val_loss: 2360.7554 - val_mae: 30.7194 - val_mape: 179.3343 - val_mse: 2333.3137 - 76s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 76s - loss: 2474.2458 - mae: 34.1385 - mape: 242.2086 - mse: 2446.1421 - val_loss: 2240.9048 - val_mae: 29.7910 - val_mape: 192.9204 - val_mse: 2212.1438 - 76s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 80s - loss: 2427.5735 - mae: 33.6006 - mape: 229.5386 - mse: 2397.9924 - val_loss: 2204.3733 - val_mae: 29.4948 - val_mape: 188.5851 - val_mse: 2173.9407 - 80s/epoch - 35ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 81s - loss: 2354.5122 - mae: 33.0971 - mape: 221.5231 - mse: 2323.2620 - val_loss: 2166.3254 - val_mae: 29.1263 - val_mape: 184.2617 - val_mse: 2134.4424 - 81s/epoch - 35ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 80s - loss: 2306.6865 - mae: 32.6861 - mape: 215.1467 - mse: 2273.9380 - val_loss: 2167.8740 - val_mae: 29.5738 - val_mape: 207.0008 - val_mse: 2134.4358 - 80s/epoch - 35ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 79s - loss: 2272.0764 - mae: 32.1697 - mape: 204.8537 - mse: 2237.7878 - val_loss: 2176.3418 - val_mae: 29.3400 - val_mape: 197.9573 - val_mse: 2141.3560 - 79s/epoch - 34ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 80s - loss: 2256.4753 - mae: 32.1830 - mape: 212.5008 - mse: 2220.5867 - val_loss: 2122.2234 - val_mae: 28.8279 - val_mape: 193.5395 - val_mse: 2085.6157 - 80s/epoch - 35ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 79s - loss: 2238.7693 - mae: 31.9735 - mape: 205.8421 - mse: 2201.4402 - val_loss: 2049.6045 - val_mae: 28.2148 - val_mape: 192.6321 - val_mse: 2011.7180 - 79s/epoch - 34ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 80s - loss: 2205.7336 - mae: 31.8015 - mape: 203.8957 - mse: 2167.1401 - val_loss: 2034.7762 - val_mae: 28.2555 - val_mape: 196.1226 - val_mse: 1995.5714 - 80s/epoch - 35ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 80s - loss: 2181.2202 - mae: 31.4755 - mape: 198.4122 - mse: 2141.3105 - val_loss: 2031.7338 - val_mae: 28.3831 - val_mape: 198.7658 - val_mse: 1991.2383 - 80s/epoch - 35ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 80s - loss: 2151.2886 - mae: 31.2136 - mape: 192.9858 - mse: 2110.1453 - val_loss: 2026.3148 - val_mae: 28.4052 - val_mape: 180.7192 - val_mse: 1984.5078 - 80s/epoch - 35ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 79s - loss: 2129.5579 - mae: 31.0120 - mape: 194.6925 - mse: 2087.0845 - val_loss: 2004.8923 - val_mae: 28.1974 - val_mape: 171.0935 - val_mse: 1961.9218 - 79s/epoch - 35ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 81s - loss: 2112.1282 - mae: 31.0364 - mape: 194.8478 - mse: 2068.5537 - val_loss: 2167.0979 - val_mae: 29.6629 - val_mape: 177.4739 - val_mse: 2123.0452 - 81s/epoch - 35ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 81s - loss: 2114.9133 - mae: 31.0438 - mape: 198.5206 - mse: 2070.3142 - val_loss: 2152.1875 - val_mae: 29.2473 - val_mape: 187.8578 - val_mse: 2107.0955 - 81s/epoch - 35ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 81s - loss: 2098.1423 - mae: 30.8455 - mape: 191.3784 - mse: 2052.5063 - val_loss: 2175.8948 - val_mae: 29.5854 - val_mape: 174.3266 - val_mse: 2129.8191 - 81s/epoch - 35ms/step\n",
      "Epoch 26/100\n",
      "2293/2293 - 78s - loss: 2063.9590 - mae: 30.4003 - mape: 189.7944 - mse: 2017.1759 - val_loss: 2163.1233 - val_mae: 29.3634 - val_mape: 170.1787 - val_mse: 2115.9414 - 78s/epoch - 34ms/step\n",
      "Epoch 27/100\n",
      "2293/2293 - 78s - loss: 2059.2209 - mae: 30.4615 - mape: 193.2773 - mse: 2011.4791 - val_loss: 2175.1050 - val_mae: 29.6770 - val_mape: 174.0754 - val_mse: 2126.9636 - 78s/epoch - 34ms/step\n",
      "Epoch 28/100\n",
      "2293/2293 - 80s - loss: 2063.0542 - mae: 30.4482 - mape: 194.1616 - mse: 2014.3207 - val_loss: 2133.9641 - val_mae: 29.4599 - val_mape: 181.8609 - val_mse: 2084.7749 - 80s/epoch - 35ms/step\n",
      "Epoch 29/100\n",
      "2293/2293 - 79s - loss: 2048.3513 - mae: 30.2522 - mape: 191.7935 - mse: 1998.4684 - val_loss: 2177.8850 - val_mae: 29.8749 - val_mape: 173.6658 - val_mse: 2127.5120 - 79s/epoch - 34ms/step\n",
      "Epoch 30/100\n",
      "2293/2293 - 78s - loss: 2045.3909 - mae: 30.1776 - mape: 188.3367 - mse: 1994.4844 - val_loss: 2157.6680 - val_mae: 29.5924 - val_mape: 181.9399 - val_mse: 2106.3730 - 78s/epoch - 34ms/step\n",
      "Epoch 31/100\n",
      "2293/2293 - 80s - loss: 2018.9215 - mae: 29.8819 - mape: 188.1183 - mse: 1967.0952 - val_loss: 2148.5344 - val_mae: 29.5337 - val_mape: 185.5555 - val_mse: 2096.3640 - 80s/epoch - 35ms/step\n",
      "Epoch 32/100\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "2293/2293 - 79s - loss: 2002.7701 - mae: 29.7291 - mape: 188.3604 - mse: 1950.0930 - val_loss: 2148.4500 - val_mae: 29.4643 - val_mape: 178.5486 - val_mse: 2095.5022 - 79s/epoch - 35ms/step\n",
      "Epoch 32: early stopping\n",
      "727/727 [==============================] - 8s 11ms/step - loss: 2752.4136 - mae: 33.9162 - mape: 249.6206 - mse: 2709.4453\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-9/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-9/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023B844743D0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 sigmoid tanh 0.2 0.1 l2 l2\n",
      "Epoch 1/100\n",
      "2293/2293 - 72s - loss: 6245.8486 - mae: 52.9741 - mape: 368.5284 - mse: 6244.3423 - val_loss: 4761.0347 - val_mae: 48.1320 - val_mape: 545.6124 - val_mse: 4759.5649 - 72s/epoch - 31ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 65s - loss: 4584.7324 - mae: 46.0904 - mape: 392.3909 - mse: 4581.7378 - val_loss: 3914.8748 - val_mae: 41.3674 - val_mape: 318.4768 - val_mse: 3911.1484 - 65s/epoch - 28ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 65s - loss: 3923.8828 - mae: 44.3699 - mape: 352.5610 - mse: 3919.6533 - val_loss: 3623.2063 - val_mae: 40.0001 - val_mape: 335.7841 - val_mse: 3618.5347 - 65s/epoch - 28ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 65s - loss: 3707.0781 - mae: 43.6954 - mape: 330.1983 - mse: 3701.8501 - val_loss: 3385.6707 - val_mae: 38.6535 - val_mape: 325.5059 - val_mse: 3379.7693 - 65s/epoch - 28ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 67s - loss: 3544.4856 - mae: 42.5239 - mape: 288.1048 - mse: 3537.7554 - val_loss: 3086.3369 - val_mae: 36.3115 - val_mape: 275.5018 - val_mse: 3078.6353 - 67s/epoch - 29ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 64s - loss: 3334.7490 - mae: 41.1161 - mape: 274.8694 - mse: 3326.2725 - val_loss: 3112.0947 - val_mae: 34.8611 - val_mape: 203.7912 - val_mse: 3102.8181 - 64s/epoch - 28ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 64s - loss: 3049.9597 - mae: 38.7133 - mape: 242.4619 - mse: 3039.7424 - val_loss: 2835.0032 - val_mae: 33.4985 - val_mape: 199.3197 - val_mse: 2823.9590 - 64s/epoch - 28ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 64s - loss: 2874.5955 - mae: 37.2214 - mape: 246.1490 - mse: 2862.7117 - val_loss: 2673.4810 - val_mae: 32.6728 - val_mape: 201.2685 - val_mse: 2660.8550 - 64s/epoch - 28ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 67s - loss: 2746.1755 - mae: 36.0899 - mape: 232.8308 - mse: 2732.8232 - val_loss: 2583.4314 - val_mae: 32.3594 - val_mape: 204.8832 - val_mse: 2569.2297 - 67s/epoch - 29ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 66s - loss: 2655.9434 - mae: 35.0568 - mape: 221.1828 - mse: 2641.0322 - val_loss: 2594.3940 - val_mae: 32.0426 - val_mape: 215.3622 - val_mse: 2578.7405 - 66s/epoch - 29ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 65s - loss: 2538.5215 - mae: 34.2295 - mape: 210.9492 - mse: 2522.1111 - val_loss: 2489.0786 - val_mae: 31.3452 - val_mape: 223.4626 - val_mse: 2471.8992 - 65s/epoch - 28ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 65s - loss: 2474.3916 - mae: 33.9766 - mape: 217.5948 - mse: 2456.3506 - val_loss: 2400.0261 - val_mae: 31.2489 - val_mape: 224.2809 - val_mse: 2381.1804 - 65s/epoch - 28ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 66s - loss: 2388.9436 - mae: 33.2009 - mape: 200.7072 - mse: 2369.3062 - val_loss: 2286.6040 - val_mae: 30.5693 - val_mape: 245.7902 - val_mse: 2266.1951 - 66s/epoch - 29ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 66s - loss: 2335.3643 - mae: 32.8858 - mape: 202.4439 - mse: 2314.2124 - val_loss: 2296.7725 - val_mae: 30.7437 - val_mape: 251.8534 - val_mse: 2274.8567 - 66s/epoch - 29ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 66s - loss: 2300.5527 - mae: 32.6273 - mape: 198.4043 - mse: 2277.9641 - val_loss: 2224.2380 - val_mae: 29.8395 - val_mape: 212.6926 - val_mse: 2200.9299 - 66s/epoch - 29ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 65s - loss: 2253.7236 - mae: 32.1959 - mape: 200.3361 - mse: 2229.6401 - val_loss: 2217.5215 - val_mae: 29.7965 - val_mape: 217.6392 - val_mse: 2192.7253 - 65s/epoch - 28ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 65s - loss: 2240.9734 - mae: 31.9894 - mape: 199.1101 - mse: 2215.5461 - val_loss: 2205.5364 - val_mae: 29.8629 - val_mape: 224.9690 - val_mse: 2179.4092 - 65s/epoch - 28ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 65s - loss: 2186.2688 - mae: 31.6301 - mape: 199.2450 - mse: 2159.4790 - val_loss: 2232.1589 - val_mae: 30.1901 - val_mape: 211.3544 - val_mse: 2204.7476 - 65s/epoch - 28ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 65s - loss: 2144.2534 - mae: 31.0559 - mape: 187.2959 - mse: 2116.3408 - val_loss: 2172.4133 - val_mae: 29.3699 - val_mape: 196.4934 - val_mse: 2143.8074 - 65s/epoch - 29ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 68s - loss: 2124.3801 - mae: 30.9294 - mape: 193.2857 - mse: 2095.1926 - val_loss: 2150.9146 - val_mae: 29.5810 - val_mape: 209.5336 - val_mse: 2121.1206 - 68s/epoch - 30ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 66s - loss: 2094.3796 - mae: 30.6632 - mape: 187.0251 - mse: 2064.0535 - val_loss: 2141.2197 - val_mae: 29.7118 - val_mape: 214.2238 - val_mse: 2110.2439 - 66s/epoch - 29ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 66s - loss: 2075.8572 - mae: 30.5511 - mape: 192.1754 - mse: 2044.3181 - val_loss: 2181.3269 - val_mae: 30.2586 - val_mape: 211.4736 - val_mse: 2149.2078 - 66s/epoch - 29ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 66s - loss: 2069.8584 - mae: 30.4525 - mape: 188.5814 - mse: 2037.1984 - val_loss: 2136.3967 - val_mae: 29.8656 - val_mape: 211.3568 - val_mse: 2103.1501 - 66s/epoch - 29ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 66s - loss: 2019.7732 - mae: 30.0262 - mape: 185.3067 - mse: 1986.0197 - val_loss: 2070.4141 - val_mae: 29.3634 - val_mape: 217.1385 - val_mse: 2036.0637 - 66s/epoch - 29ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 66s - loss: 2005.0684 - mae: 29.8435 - mape: 190.9478 - mse: 1970.1995 - val_loss: 2035.9437 - val_mae: 29.1393 - val_mape: 223.3687 - val_mse: 2000.4749 - 66s/epoch - 29ms/step\n",
      "Epoch 26/100\n",
      "2293/2293 - 67s - loss: 1999.4285 - mae: 29.8130 - mape: 191.0042 - mse: 1963.3241 - val_loss: 1952.9203 - val_mae: 28.7565 - val_mape: 232.9642 - val_mse: 1916.1464 - 67s/epoch - 29ms/step\n",
      "Epoch 27/100\n",
      "2293/2293 - 67s - loss: 1999.5781 - mae: 29.7792 - mape: 188.3036 - mse: 1962.1670 - val_loss: 1891.7216 - val_mae: 27.7805 - val_mape: 212.3870 - val_mse: 1853.6233 - 67s/epoch - 29ms/step\n",
      "Epoch 28/100\n",
      "2293/2293 - 68s - loss: 1990.2573 - mae: 29.7300 - mape: 183.9834 - mse: 1951.6827 - val_loss: 1962.4614 - val_mae: 28.2082 - val_mape: 215.2240 - val_mse: 1923.3488 - 68s/epoch - 30ms/step\n",
      "Epoch 29/100\n",
      "2293/2293 - 68s - loss: 1968.6368 - mae: 29.5385 - mape: 181.4531 - mse: 1928.9651 - val_loss: 1984.8934 - val_mae: 28.1224 - val_mape: 202.1667 - val_mse: 1944.6184 - 68s/epoch - 30ms/step\n",
      "Epoch 30/100\n",
      "2293/2293 - 68s - loss: 1928.1483 - mae: 29.1565 - mape: 176.5996 - mse: 1887.3650 - val_loss: 2042.2401 - val_mae: 28.1447 - val_mape: 181.3827 - val_mse: 2000.9563 - 68s/epoch - 30ms/step\n",
      "Epoch 31/100\n",
      "2293/2293 - 67s - loss: 1919.5699 - mae: 29.0804 - mape: 179.5377 - mse: 1877.7118 - val_loss: 2037.0437 - val_mae: 28.4479 - val_mape: 176.4187 - val_mse: 1994.6368 - 67s/epoch - 29ms/step\n",
      "Epoch 32/100\n",
      "2293/2293 - 68s - loss: 1905.4319 - mae: 28.9762 - mape: 175.6682 - mse: 1862.5039 - val_loss: 2063.9275 - val_mae: 28.8487 - val_mape: 179.6957 - val_mse: 2020.4663 - 68s/epoch - 29ms/step\n",
      "Epoch 33/100\n",
      "2293/2293 - 69s - loss: 1888.9155 - mae: 28.9116 - mape: 178.0443 - mse: 1844.9263 - val_loss: 2161.6655 - val_mae: 29.3185 - val_mape: 179.8286 - val_mse: 2117.1062 - 69s/epoch - 30ms/step\n",
      "Epoch 34/100\n",
      "2293/2293 - 69s - loss: 1887.3016 - mae: 28.8503 - mape: 184.8881 - mse: 1842.1829 - val_loss: 2184.9216 - val_mae: 29.2815 - val_mape: 168.5614 - val_mse: 2139.1643 - 69s/epoch - 30ms/step\n",
      "Epoch 35/100\n",
      "2293/2293 - 68s - loss: 1863.1757 - mae: 28.5514 - mape: 179.8888 - mse: 1816.8940 - val_loss: 2250.7866 - val_mae: 29.8615 - val_mape: 179.5510 - val_mse: 2203.9692 - 68s/epoch - 30ms/step\n",
      "Epoch 36/100\n",
      "2293/2293 - 69s - loss: 1857.7369 - mae: 28.4678 - mape: 183.7040 - mse: 1810.4542 - val_loss: 2307.5356 - val_mae: 30.5690 - val_mape: 182.7994 - val_mse: 2259.7761 - 69s/epoch - 30ms/step\n",
      "Epoch 37/100\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "2293/2293 - 68s - loss: 1852.6829 - mae: 28.4379 - mape: 185.1751 - mse: 1804.5096 - val_loss: 2248.2844 - val_mae: 29.9567 - val_mape: 178.3385 - val_mse: 2199.7378 - 68s/epoch - 29ms/step\n",
      "Epoch 37: early stopping\n",
      "727/727 [==============================] - 7s 9ms/step - loss: 2601.4875 - mae: 33.8753 - mape: 226.8044 - mse: 2563.3853\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-10/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-10/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023C3A35BEE0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 sigmoid tanh 0.2 0.1 l2 l1l2\n",
      "Epoch 1/100\n",
      "2293/2293 - 79s - loss: 6372.2217 - mae: 53.4702 - mape: 355.5881 - mse: 6371.4028 - val_loss: 4886.1035 - val_mae: 48.6434 - val_mape: 544.4028 - val_mse: 4885.3936 - 79s/epoch - 34ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 77s - loss: 4799.5322 - mae: 47.0175 - mape: 401.9298 - mse: 4797.9175 - val_loss: 3847.5420 - val_mae: 41.3851 - val_mape: 378.3502 - val_mse: 3845.3679 - 77s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 78s - loss: 3934.5769 - mae: 44.0530 - mape: 347.0302 - mse: 3932.0879 - val_loss: 3712.8311 - val_mae: 40.6867 - val_mape: 379.7539 - val_mse: 3709.9517 - 78s/epoch - 34ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 79s - loss: 3738.6794 - mae: 43.6385 - mape: 332.7751 - mse: 3735.4521 - val_loss: 3403.6877 - val_mae: 38.0511 - val_mape: 278.7223 - val_mse: 3399.9131 - 79s/epoch - 34ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 78s - loss: 3533.2617 - mae: 42.6184 - mape: 291.3043 - mse: 3528.8401 - val_loss: 3066.8325 - val_mae: 35.7047 - val_mape: 264.2362 - val_mse: 3061.6807 - 78s/epoch - 34ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 79s - loss: 3268.0190 - mae: 40.3654 - mape: 243.1221 - mse: 3262.1609 - val_loss: 2891.7517 - val_mae: 34.4750 - val_mape: 257.5804 - val_mse: 2885.1228 - 79s/epoch - 34ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 78s - loss: 2963.6609 - mae: 38.0767 - mape: 236.2093 - mse: 2956.3486 - val_loss: 2682.7830 - val_mae: 33.4673 - val_mape: 264.8351 - val_mse: 2674.7676 - 78s/epoch - 34ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 78s - loss: 2804.1226 - mae: 36.8930 - mape: 228.6612 - mse: 2795.4119 - val_loss: 2526.8496 - val_mae: 32.1819 - val_mape: 227.7371 - val_mse: 2517.4016 - 78s/epoch - 34ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 79s - loss: 2655.5107 - mae: 35.3724 - mape: 217.9284 - mse: 2645.3926 - val_loss: 2442.3989 - val_mae: 31.6590 - val_mape: 230.5746 - val_mse: 2431.5828 - 79s/epoch - 34ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 81s - loss: 2526.3845 - mae: 34.3324 - mape: 219.6902 - mse: 2514.9109 - val_loss: 2416.5842 - val_mae: 30.6639 - val_mape: 178.3036 - val_mse: 2404.4775 - 81s/epoch - 35ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 81s - loss: 2396.1631 - mae: 33.2778 - mape: 214.3008 - mse: 2383.4407 - val_loss: 2207.6526 - val_mae: 29.6692 - val_mape: 202.6577 - val_mse: 2194.2932 - 81s/epoch - 35ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 80s - loss: 2323.5137 - mae: 32.8450 - mape: 202.4091 - mse: 2309.5825 - val_loss: 2286.7739 - val_mae: 30.4988 - val_mape: 215.9206 - val_mse: 2272.2048 - 80s/epoch - 35ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 78s - loss: 2282.2236 - mae: 32.4173 - mape: 201.8088 - mse: 2267.1355 - val_loss: 2388.2627 - val_mae: 30.6832 - val_mape: 212.6935 - val_mse: 2372.5664 - 78s/epoch - 34ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 78s - loss: 2253.5461 - mae: 32.2358 - mape: 193.2147 - mse: 2237.4387 - val_loss: 2368.3154 - val_mae: 30.5482 - val_mape: 201.8815 - val_mse: 2351.6604 - 78s/epoch - 34ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 79s - loss: 2225.9675 - mae: 32.0069 - mape: 200.7175 - mse: 2208.8987 - val_loss: 2301.2261 - val_mae: 30.1621 - val_mape: 190.8434 - val_mse: 2283.6130 - 79s/epoch - 34ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 78s - loss: 2193.4060 - mae: 31.5982 - mape: 190.3229 - mse: 2175.3799 - val_loss: 2274.6836 - val_mae: 30.2387 - val_mape: 197.8773 - val_mse: 2256.1692 - 78s/epoch - 34ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 77s - loss: 2159.0566 - mae: 31.3121 - mape: 194.3386 - mse: 2140.1089 - val_loss: 2183.9124 - val_mae: 29.4860 - val_mape: 190.8082 - val_mse: 2164.4363 - 77s/epoch - 34ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 77s - loss: 2130.5867 - mae: 31.0028 - mape: 186.6322 - mse: 2110.6741 - val_loss: 2135.9927 - val_mae: 29.0870 - val_mape: 192.3457 - val_mse: 2115.5806 - 77s/epoch - 34ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 78s - loss: 2098.8440 - mae: 30.7374 - mape: 185.4480 - mse: 2078.0110 - val_loss: 2115.5281 - val_mae: 28.8905 - val_mape: 192.7534 - val_mse: 2094.2578 - 78s/epoch - 34ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 79s - loss: 2067.3613 - mae: 30.4785 - mape: 185.3067 - mse: 2045.6624 - val_loss: 2172.8960 - val_mae: 29.4104 - val_mape: 197.2531 - val_mse: 2150.8093 - 79s/epoch - 34ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 79s - loss: 2066.1509 - mae: 30.4779 - mape: 185.1040 - mse: 2043.7031 - val_loss: 2186.7188 - val_mae: 29.6375 - val_mape: 202.0547 - val_mse: 2163.9365 - 79s/epoch - 34ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 79s - loss: 2064.0610 - mae: 30.2767 - mape: 183.0860 - mse: 2040.9442 - val_loss: 2168.6492 - val_mae: 29.6119 - val_mape: 206.4911 - val_mse: 2145.1953 - 79s/epoch - 34ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 79s - loss: 2038.2930 - mae: 30.1044 - mape: 182.1032 - mse: 2014.5040 - val_loss: 2130.8484 - val_mae: 29.5257 - val_mape: 201.1592 - val_mse: 2106.7417 - 79s/epoch - 34ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 78s - loss: 2011.0249 - mae: 29.8277 - mape: 179.7864 - mse: 1986.5515 - val_loss: 2078.9365 - val_mae: 29.3395 - val_mape: 202.1830 - val_mse: 2054.0725 - 78s/epoch - 34ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 77s - loss: 1984.8513 - mae: 29.5637 - mape: 174.2900 - mse: 1959.6744 - val_loss: 2028.1321 - val_mae: 29.1876 - val_mape: 202.1580 - val_mse: 2002.5969 - 77s/epoch - 34ms/step\n",
      "Epoch 26/100\n",
      "2293/2293 - 77s - loss: 1966.4575 - mae: 29.4293 - mape: 175.7543 - mse: 1940.6204 - val_loss: 2046.5031 - val_mae: 29.4687 - val_mape: 200.5652 - val_mse: 2020.3260 - 77s/epoch - 34ms/step\n",
      "Epoch 27/100\n",
      "2293/2293 - 78s - loss: 1969.6224 - mae: 29.4855 - mape: 178.6408 - mse: 1943.1454 - val_loss: 2168.1021 - val_mae: 30.4548 - val_mape: 206.7082 - val_mse: 2141.2996 - 78s/epoch - 34ms/step\n",
      "Epoch 28/100\n",
      "2293/2293 - 77s - loss: 2010.0413 - mae: 29.9455 - mape: 184.1768 - mse: 1982.9268 - val_loss: 2249.8293 - val_mae: 30.9593 - val_mape: 212.6127 - val_mse: 2222.3496 - 77s/epoch - 34ms/step\n",
      "Epoch 29/100\n",
      "2293/2293 - 78s - loss: 1990.1484 - mae: 29.7466 - mape: 183.7111 - mse: 1962.3151 - val_loss: 2140.9216 - val_mae: 29.8218 - val_mape: 215.3835 - val_mse: 2112.6560 - 78s/epoch - 34ms/step\n",
      "Epoch 30/100\n",
      "2293/2293 - 78s - loss: 2021.3982 - mae: 29.9654 - mape: 184.0622 - mse: 1992.7865 - val_loss: 2160.8015 - val_mae: 29.8686 - val_mape: 238.0196 - val_mse: 2131.7275 - 78s/epoch - 34ms/step\n",
      "Epoch 31/100\n",
      "2293/2293 - 78s - loss: 2025.1610 - mae: 29.9234 - mape: 182.2954 - mse: 1995.8690 - val_loss: 2133.3289 - val_mae: 29.2283 - val_mape: 214.9141 - val_mse: 2103.6941 - 78s/epoch - 34ms/step\n",
      "Epoch 32/100\n",
      "2293/2293 - 77s - loss: 1989.6257 - mae: 29.7230 - mape: 183.6784 - mse: 1959.7645 - val_loss: 2242.4023 - val_mae: 29.5978 - val_mape: 203.9343 - val_mse: 2212.1682 - 77s/epoch - 34ms/step\n",
      "Epoch 33/100\n",
      "2293/2293 - 77s - loss: 1945.0660 - mae: 29.3167 - mape: 175.5889 - mse: 1914.6292 - val_loss: 2199.3540 - val_mae: 29.3188 - val_mape: 208.8665 - val_mse: 2168.5286 - 77s/epoch - 34ms/step\n",
      "Epoch 34/100\n",
      "2293/2293 - 77s - loss: 1952.7399 - mae: 29.2025 - mape: 182.9423 - mse: 1921.8053 - val_loss: 2397.5195 - val_mae: 30.7563 - val_mape: 196.5797 - val_mse: 2366.2507 - 77s/epoch - 34ms/step\n",
      "Epoch 35/100\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "2293/2293 - 77s - loss: 1940.1777 - mae: 29.1465 - mape: 175.9598 - mse: 1908.7029 - val_loss: 2462.3098 - val_mae: 31.3371 - val_mape: 207.0875 - val_mse: 2430.5420 - 77s/epoch - 34ms/step\n",
      "Epoch 35: early stopping\n",
      "727/727 [==============================] - 7s 9ms/step - loss: 2837.1699 - mae: 34.8043 - mape: 244.8456 - mse: 2811.6343\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-11/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-11/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023B98D8C4C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 sigmoid tanh 0.2 0.1 l1l2 None\n",
      "Epoch 1/100\n",
      "2293/2293 - 77s - loss: 6268.6992 - mae: 53.0596 - mape: 366.9453 - mse: 6268.6992 - val_loss: 4836.1748 - val_mae: 48.9106 - val_mape: 573.2996 - val_mse: 4836.1748 - 77s/epoch - 34ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 74s - loss: 4697.2036 - mae: 47.8785 - mape: 489.1623 - mse: 4697.2036 - val_loss: 3763.6184 - val_mae: 40.9873 - val_mape: 378.3381 - val_mse: 3763.6184 - 74s/epoch - 32ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 74s - loss: 3914.9270 - mae: 44.3479 - mape: 349.4515 - mse: 3914.9270 - val_loss: 3678.7834 - val_mae: 39.8349 - val_mape: 290.3958 - val_mse: 3678.7834 - 74s/epoch - 32ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 74s - loss: 3757.2661 - mae: 43.5105 - mape: 320.7350 - mse: 3757.2661 - val_loss: 3385.3481 - val_mae: 38.0249 - val_mape: 290.6015 - val_mse: 3385.3481 - 74s/epoch - 32ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 74s - loss: 3501.2634 - mae: 42.0503 - mape: 306.9611 - mse: 3501.2634 - val_loss: 3110.8450 - val_mae: 35.8101 - val_mape: 268.5281 - val_mse: 3110.8450 - 74s/epoch - 32ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 74s - loss: 3217.6211 - mae: 40.0209 - mape: 261.5146 - mse: 3217.6211 - val_loss: 2940.6519 - val_mae: 34.4397 - val_mape: 226.9634 - val_mse: 2940.6519 - 74s/epoch - 32ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 75s - loss: 2982.3669 - mae: 38.2857 - mape: 242.2195 - mse: 2982.3669 - val_loss: 2716.0889 - val_mae: 32.9504 - val_mape: 213.6834 - val_mse: 2716.0889 - 75s/epoch - 33ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 76s - loss: 2857.7756 - mae: 37.1000 - mape: 246.1704 - mse: 2857.7756 - val_loss: 2621.8379 - val_mae: 32.9577 - val_mape: 226.3226 - val_mse: 2621.8379 - 76s/epoch - 33ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 76s - loss: 2778.3608 - mae: 36.1821 - mape: 228.7651 - mse: 2778.3608 - val_loss: 2429.8245 - val_mae: 31.5951 - val_mape: 235.2181 - val_mse: 2429.8245 - 76s/epoch - 33ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 76s - loss: 2564.5874 - mae: 34.5939 - mape: 223.3542 - mse: 2564.5874 - val_loss: 2325.5454 - val_mae: 31.0955 - val_mape: 251.5869 - val_mse: 2325.5454 - 76s/epoch - 33ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 76s - loss: 2405.0298 - mae: 33.3334 - mape: 212.8464 - mse: 2405.0298 - val_loss: 2271.3438 - val_mae: 30.6179 - val_mape: 242.3823 - val_mse: 2271.3438 - 76s/epoch - 33ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 77s - loss: 2350.7217 - mae: 33.0255 - mape: 210.4386 - mse: 2350.7217 - val_loss: 2239.4395 - val_mae: 30.3038 - val_mape: 216.3983 - val_mse: 2239.4395 - 77s/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 77s - loss: 2305.1694 - mae: 32.7879 - mape: 207.6748 - mse: 2305.1694 - val_loss: 2333.2417 - val_mae: 30.0914 - val_mape: 191.8371 - val_mse: 2333.2417 - 77s/epoch - 33ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 77s - loss: 2272.3086 - mae: 32.5566 - mape: 202.0172 - mse: 2272.3086 - val_loss: 2208.8706 - val_mae: 29.4827 - val_mape: 218.0087 - val_mse: 2208.8706 - 77s/epoch - 33ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 77s - loss: 2208.0933 - mae: 31.9833 - mape: 198.2115 - mse: 2208.0933 - val_loss: 2118.4900 - val_mae: 28.8857 - val_mape: 189.0162 - val_mse: 2118.4900 - 77s/epoch - 34ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 76s - loss: 2174.6479 - mae: 31.7058 - mape: 197.1045 - mse: 2174.6479 - val_loss: 2134.1885 - val_mae: 29.4123 - val_mape: 185.2499 - val_mse: 2134.1885 - 76s/epoch - 33ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 77s - loss: 2128.4673 - mae: 31.3656 - mape: 194.0749 - mse: 2128.4673 - val_loss: 2110.6768 - val_mae: 29.3726 - val_mape: 191.5831 - val_mse: 2110.6768 - 77s/epoch - 34ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 78s - loss: 2088.1707 - mae: 31.0588 - mape: 191.7111 - mse: 2088.1707 - val_loss: 1962.7498 - val_mae: 28.6322 - val_mape: 212.3894 - val_mse: 1962.7498 - 78s/epoch - 34ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 78s - loss: 2065.5259 - mae: 30.8924 - mape: 186.8853 - mse: 2065.5259 - val_loss: 1996.8260 - val_mae: 29.0350 - val_mape: 221.9706 - val_mse: 1996.8260 - 78s/epoch - 34ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 78s - loss: 2030.6385 - mae: 30.5760 - mape: 183.9192 - mse: 2030.6385 - val_loss: 2016.5729 - val_mae: 29.0049 - val_mape: 208.4373 - val_mse: 2016.5729 - 78s/epoch - 34ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 78s - loss: 2021.3711 - mae: 30.5147 - mape: 187.2838 - mse: 2021.3711 - val_loss: 1995.4768 - val_mae: 28.8180 - val_mape: 203.9255 - val_mse: 1995.4768 - 78s/epoch - 34ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 79s - loss: 2010.0099 - mae: 30.4248 - mape: 185.9582 - mse: 2010.0099 - val_loss: 2022.2358 - val_mae: 28.6377 - val_mape: 200.4103 - val_mse: 2022.2358 - 79s/epoch - 34ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 79s - loss: 1976.4172 - mae: 30.0510 - mape: 186.8035 - mse: 1976.4172 - val_loss: 2046.5642 - val_mae: 28.7134 - val_mape: 186.8979 - val_mse: 2046.5642 - 79s/epoch - 34ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 79s - loss: 1967.0977 - mae: 29.8325 - mape: 187.0471 - mse: 1967.0977 - val_loss: 2036.7542 - val_mae: 28.9780 - val_mape: 188.8582 - val_mse: 2036.7542 - 79s/epoch - 34ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 81s - loss: 1936.9327 - mae: 29.5156 - mape: 184.0271 - mse: 1936.9327 - val_loss: 2086.7971 - val_mae: 29.4923 - val_mape: 187.7607 - val_mse: 2086.7971 - 81s/epoch - 36ms/step\n",
      "Epoch 26/100\n",
      "2293/2293 - 80s - loss: 1962.2013 - mae: 29.7556 - mape: 186.7935 - mse: 1962.2013 - val_loss: 2158.0859 - val_mae: 29.9994 - val_mape: 196.5020 - val_mse: 2158.0859 - 80s/epoch - 35ms/step\n",
      "Epoch 27/100\n",
      "2293/2293 - 82s - loss: 1944.3937 - mae: 29.6098 - mape: 189.8527 - mse: 1944.3937 - val_loss: 2266.0864 - val_mae: 30.2879 - val_mape: 189.2798 - val_mse: 2266.0864 - 82s/epoch - 36ms/step\n",
      "Epoch 28/100\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "2293/2293 - 82s - loss: 1919.1311 - mae: 29.3918 - mape: 183.3385 - mse: 1919.1311 - val_loss: 2338.3967 - val_mae: 30.4173 - val_mape: 191.1578 - val_mse: 2338.3967 - 82s/epoch - 36ms/step\n",
      "Epoch 28: early stopping\n",
      "727/727 [==============================] - 7s 10ms/step - loss: 2592.2571 - mae: 33.1637 - mape: 257.4023 - mse: 2592.2571\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-12/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-12/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023C3AE832B0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 sigmoid tanh 0.2 0.1 l1l2 l1\n",
      "Epoch 1/100\n",
      "2293/2293 - 100s - loss: 6248.4028 - mae: 52.8150 - mape: 353.3623 - mse: 6245.3091 - val_loss: 4715.6064 - val_mae: 46.6042 - val_mape: 454.3424 - val_mse: 4712.6470 - 100s/epoch - 44ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 98s - loss: 4498.6699 - mae: 45.7306 - mape: 374.1147 - mse: 4493.6074 - val_loss: 3772.0012 - val_mae: 40.0423 - val_mape: 309.0788 - val_mse: 3766.4824 - 98s/epoch - 43ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 99s - loss: 3891.2490 - mae: 44.2810 - mape: 342.7086 - mse: 3885.4680 - val_loss: 3493.3140 - val_mae: 39.4306 - val_mape: 335.7402 - val_mse: 3487.4390 - 99s/epoch - 43ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 98s - loss: 3657.4316 - mae: 43.3825 - mape: 336.0766 - mse: 3651.2646 - val_loss: 3211.9932 - val_mae: 37.8941 - val_mape: 340.2547 - val_mse: 3205.7556 - 98s/epoch - 43ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 97s - loss: 3477.9766 - mae: 42.0522 - mape: 312.3921 - mse: 3471.3323 - val_loss: 2953.3655 - val_mae: 35.1577 - val_mape: 278.6272 - val_mse: 2946.5132 - 97s/epoch - 42ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 99s - loss: 3230.5784 - mae: 40.5503 - mape: 292.9142 - mse: 3223.4744 - val_loss: 2729.4968 - val_mae: 33.6271 - val_mape: 237.8887 - val_mse: 2722.2046 - 99s/epoch - 43ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 100s - loss: 2995.2971 - mae: 38.7169 - mape: 269.7706 - mse: 2987.7200 - val_loss: 2632.7900 - val_mae: 32.9480 - val_mape: 220.4766 - val_mse: 2624.9246 - 100s/epoch - 44ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 98s - loss: 2814.9705 - mae: 37.1299 - mape: 258.3897 - mse: 2806.8667 - val_loss: 2618.6038 - val_mae: 32.8539 - val_mape: 214.6599 - val_mse: 2610.3796 - 98s/epoch - 43ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 98s - loss: 2683.0652 - mae: 36.0543 - mape: 252.1266 - mse: 2674.6489 - val_loss: 2479.4355 - val_mae: 32.1016 - val_mape: 235.2803 - val_mse: 2470.8557 - 98s/epoch - 43ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 96s - loss: 2561.5969 - mae: 34.8716 - mape: 237.4850 - mse: 2552.8154 - val_loss: 2331.3606 - val_mae: 30.7991 - val_mape: 227.7497 - val_mse: 2322.1919 - 96s/epoch - 42ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 97s - loss: 2481.1165 - mae: 34.0897 - mape: 227.1579 - mse: 2471.7527 - val_loss: 2290.0437 - val_mae: 30.3458 - val_mape: 213.4196 - val_mse: 2280.3943 - 97s/epoch - 42ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 98s - loss: 2384.4995 - mae: 33.4811 - mape: 231.1288 - mse: 2374.7607 - val_loss: 2162.0088 - val_mae: 29.4450 - val_mape: 202.1355 - val_mse: 2152.0261 - 98s/epoch - 43ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 92s - loss: 2318.4382 - mae: 32.8488 - mape: 220.5096 - mse: 2308.3928 - val_loss: 2138.1921 - val_mae: 29.2264 - val_mape: 201.3540 - val_mse: 2127.9258 - 92s/epoch - 40ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 92s - loss: 2267.0283 - mae: 32.5278 - mape: 224.5687 - mse: 2256.6428 - val_loss: 2077.2170 - val_mae: 28.6598 - val_mape: 190.4925 - val_mse: 2066.6306 - 92s/epoch - 40ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 92s - loss: 2229.1738 - mae: 32.0776 - mape: 216.3581 - mse: 2218.4958 - val_loss: 2090.2043 - val_mae: 28.6867 - val_mape: 185.6154 - val_mse: 2079.3149 - 92s/epoch - 40ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 92s - loss: 2192.2759 - mae: 31.7343 - mape: 209.5722 - mse: 2181.2966 - val_loss: 2094.5593 - val_mae: 28.8047 - val_mape: 195.6667 - val_mse: 2083.3286 - 92s/epoch - 40ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 92s - loss: 2156.0449 - mae: 31.4218 - mape: 205.3053 - mse: 2144.7378 - val_loss: 2097.9895 - val_mae: 28.9773 - val_mape: 197.5141 - val_mse: 2086.4106 - 92s/epoch - 40ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 92s - loss: 2119.0408 - mae: 31.1337 - mape: 201.3599 - mse: 2107.4304 - val_loss: 2028.3906 - val_mae: 28.7816 - val_mape: 225.5081 - val_mse: 2016.5385 - 92s/epoch - 40ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 92s - loss: 2087.8713 - mae: 30.9648 - mape: 200.9087 - mse: 2075.9763 - val_loss: 2038.1997 - val_mae: 29.1930 - val_mape: 220.6493 - val_mse: 2026.0177 - 92s/epoch - 40ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 93s - loss: 2074.0271 - mae: 30.8412 - mape: 196.7832 - mse: 2061.8037 - val_loss: 2002.5396 - val_mae: 28.9213 - val_mape: 218.8663 - val_mse: 1990.1379 - 93s/epoch - 40ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 93s - loss: 2056.2251 - mae: 30.6317 - mape: 194.7952 - mse: 2043.7638 - val_loss: 2008.9011 - val_mae: 28.9817 - val_mape: 210.4361 - val_mse: 1996.2545 - 93s/epoch - 40ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 92s - loss: 2044.1865 - mae: 30.5433 - mape: 193.4206 - mse: 2031.4609 - val_loss: 2021.3071 - val_mae: 29.0626 - val_mape: 200.9810 - val_mse: 2008.4266 - 92s/epoch - 40ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 92s - loss: 2041.2478 - mae: 30.5967 - mape: 195.0593 - mse: 2028.2946 - val_loss: 2020.6217 - val_mae: 29.0313 - val_mape: 221.2728 - val_mse: 2007.4691 - 92s/epoch - 40ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 92s - loss: 2026.0223 - mae: 30.5271 - mape: 195.8686 - mse: 2012.7549 - val_loss: 2062.3599 - val_mae: 29.2482 - val_mape: 213.4513 - val_mse: 2048.9666 - 92s/epoch - 40ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 92s - loss: 2008.0332 - mae: 30.2672 - mape: 195.6584 - mse: 1994.5157 - val_loss: 2016.2291 - val_mae: 28.7298 - val_mape: 210.4928 - val_mse: 2002.5237 - 92s/epoch - 40ms/step\n",
      "Epoch 26/100\n",
      "2293/2293 - 92s - loss: 1988.5760 - mae: 30.1194 - mape: 197.8888 - mse: 1974.7205 - val_loss: 2069.4683 - val_mae: 29.1387 - val_mape: 195.3740 - val_mse: 2055.4546 - 92s/epoch - 40ms/step\n",
      "Epoch 27/100\n",
      "2293/2293 - 92s - loss: 1969.8533 - mae: 29.9195 - mape: 194.2873 - mse: 1955.6997 - val_loss: 2076.5112 - val_mae: 29.0849 - val_mape: 182.7607 - val_mse: 2062.2014 - 92s/epoch - 40ms/step\n",
      "Epoch 28/100\n",
      "2293/2293 - 92s - loss: 1962.2992 - mae: 29.8130 - mape: 190.5592 - mse: 1947.8346 - val_loss: 2105.7166 - val_mae: 29.3479 - val_mape: 178.4527 - val_mse: 2091.0913 - 92s/epoch - 40ms/step\n",
      "Epoch 29/100\n",
      "2293/2293 - 92s - loss: 1956.2404 - mae: 29.7069 - mape: 188.6715 - mse: 1941.4893 - val_loss: 2118.3418 - val_mae: 29.4318 - val_mape: 180.7642 - val_mse: 2103.4355 - 92s/epoch - 40ms/step\n",
      "Epoch 30/100\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "2293/2293 - 92s - loss: 1938.6001 - mae: 29.5517 - mape: 186.4473 - mse: 1923.5605 - val_loss: 2144.3943 - val_mae: 29.6260 - val_mape: 195.3490 - val_mse: 2129.1770 - 92s/epoch - 40ms/step\n",
      "Epoch 30: early stopping\n",
      "727/727 [==============================] - 8s 11ms/step - loss: 2680.8665 - mae: 33.5448 - mape: 257.2087 - mse: 2668.4651\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-13/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-13/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023C3AB53880> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 sigmoid tanh 0.2 0.1 l1l2 l2\n",
      "Epoch 1/100\n",
      "2293/2293 - 94s - loss: 6369.2607 - mae: 53.4103 - mape: 351.9267 - mse: 6368.6592 - val_loss: 4828.9390 - val_mae: 48.4616 - val_mape: 549.5345 - val_mse: 4828.4756 - 94s/epoch - 41ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 90s - loss: 4646.8599 - mae: 47.0596 - mape: 450.6821 - mse: 4645.7485 - val_loss: 3902.6729 - val_mae: 41.2897 - val_mape: 361.9279 - val_mse: 3901.2246 - 90s/epoch - 39ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 90s - loss: 3998.3962 - mae: 45.0045 - mape: 402.5344 - mse: 3996.8230 - val_loss: 3827.5483 - val_mae: 40.4924 - val_mape: 318.1877 - val_mse: 3825.8379 - 90s/epoch - 39ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 89s - loss: 3796.0256 - mae: 44.3317 - mape: 323.7347 - mse: 3794.1602 - val_loss: 4163.9072 - val_mae: 41.4424 - val_mape: 280.6424 - val_mse: 4161.9917 - 89s/epoch - 39ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 90s - loss: 3831.9641 - mae: 43.1970 - mape: 315.5282 - mse: 3829.8323 - val_loss: 3774.7622 - val_mae: 38.9677 - val_mape: 249.5286 - val_mse: 3772.3174 - 90s/epoch - 39ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 89s - loss: 3520.4558 - mae: 42.4194 - mape: 299.1806 - mse: 3517.9771 - val_loss: 4400.3901 - val_mae: 44.5271 - val_mape: 184.6847 - val_mse: 4397.8506 - 89s/epoch - 39ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 89s - loss: 3289.7644 - mae: 40.8510 - mape: 300.0689 - mse: 3287.0405 - val_loss: 3109.9141 - val_mae: 35.2757 - val_mape: 224.6787 - val_mse: 3107.0747 - 89s/epoch - 39ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 90s - loss: 3057.3320 - mae: 38.9823 - mape: 264.4278 - mse: 3054.4070 - val_loss: 2921.1331 - val_mae: 34.5004 - val_mape: 211.5457 - val_mse: 2918.1514 - 90s/epoch - 39ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 90s - loss: 2915.1086 - mae: 37.7422 - mape: 255.4549 - mse: 2911.9531 - val_loss: 2799.0283 - val_mae: 34.0491 - val_mape: 217.7085 - val_mse: 2795.6943 - 90s/epoch - 39ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 90s - loss: 2730.0623 - mae: 36.1729 - mape: 241.7336 - mse: 2726.6641 - val_loss: 2601.7273 - val_mae: 33.1460 - val_mape: 237.7997 - val_mse: 2598.2534 - 90s/epoch - 39ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 90s - loss: 2613.8154 - mae: 35.2054 - mape: 241.9847 - mse: 2610.1836 - val_loss: 2529.2100 - val_mae: 31.6772 - val_mape: 185.7738 - val_mse: 2525.3909 - 90s/epoch - 39ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 90s - loss: 2551.6250 - mae: 34.5829 - mape: 232.5178 - mse: 2547.5447 - val_loss: 2338.1389 - val_mae: 31.0002 - val_mape: 224.6177 - val_mse: 2333.9348 - 90s/epoch - 39ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 90s - loss: 2419.3979 - mae: 33.5813 - mape: 219.3599 - mse: 2415.1006 - val_loss: 2282.7371 - val_mae: 30.4711 - val_mape: 195.5959 - val_mse: 2278.2039 - 90s/epoch - 39ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 90s - loss: 2361.1206 - mae: 33.0324 - mape: 206.6670 - mse: 2356.5059 - val_loss: 2204.0286 - val_mae: 29.5803 - val_mape: 182.9303 - val_mse: 2199.2615 - 90s/epoch - 39ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 90s - loss: 2290.3984 - mae: 32.4844 - mape: 202.2905 - mse: 2285.5505 - val_loss: 2208.9290 - val_mae: 29.6253 - val_mape: 174.8490 - val_mse: 2203.9246 - 90s/epoch - 39ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 90s - loss: 2228.4207 - mae: 32.0102 - mape: 197.2692 - mse: 2223.2854 - val_loss: 2223.4753 - val_mae: 29.6622 - val_mape: 174.1814 - val_mse: 2218.1411 - 90s/epoch - 39ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 90s - loss: 2205.9387 - mae: 31.7750 - mape: 197.7540 - mse: 2200.4351 - val_loss: 2211.1934 - val_mae: 29.6817 - val_mape: 179.9512 - val_mse: 2205.4980 - 90s/epoch - 39ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 89s - loss: 2257.8674 - mae: 32.3408 - mape: 191.7623 - mse: 2252.0073 - val_loss: 2286.8035 - val_mae: 29.9813 - val_mape: 175.6514 - val_mse: 2280.6650 - 89s/epoch - 39ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 90s - loss: 2175.4041 - mae: 31.6068 - mape: 191.1861 - mse: 2169.1492 - val_loss: 2205.6499 - val_mae: 29.4866 - val_mape: 184.1211 - val_mse: 2199.2217 - 90s/epoch - 39ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 89s - loss: 2123.5649 - mae: 31.1441 - mape: 191.4630 - mse: 2116.9714 - val_loss: 2206.9070 - val_mae: 29.3239 - val_mape: 171.9353 - val_mse: 2200.0942 - 89s/epoch - 39ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 90s - loss: 2076.5288 - mae: 30.6855 - mape: 187.1257 - mse: 2069.4890 - val_loss: 2215.5684 - val_mae: 29.3548 - val_mape: 176.3027 - val_mse: 2208.2581 - 90s/epoch - 39ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 90s - loss: 2046.1414 - mae: 30.4439 - mape: 186.3271 - mse: 2038.6001 - val_loss: 2237.9744 - val_mae: 29.3588 - val_mape: 177.4547 - val_mse: 2230.1401 - 90s/epoch - 39ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 90s - loss: 2067.0500 - mae: 30.7970 - mape: 192.0220 - mse: 2059.0686 - val_loss: 2250.1143 - val_mae: 30.0048 - val_mape: 200.6460 - val_mse: 2241.9282 - 90s/epoch - 39ms/step\n",
      "Epoch 24/100\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "2293/2293 - 90s - loss: 2010.0869 - mae: 30.3771 - mape: 187.8402 - mse: 2001.7719 - val_loss: 2308.0632 - val_mae: 30.3920 - val_mape: 196.9912 - val_mse: 2299.5410 - 90s/epoch - 39ms/step\n",
      "Epoch 24: early stopping\n",
      "727/727 [==============================] - 8s 11ms/step - loss: 2882.2385 - mae: 34.7304 - mape: 266.8831 - mse: 2877.4712\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-14/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-14/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023C3AD08220> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 sigmoid tanh 0.2 0.1 l1l2 l1l2\n",
      "Epoch 1/100\n",
      "2293/2293 - 79s - loss: 6253.3706 - mae: 52.9753 - mape: 365.3383 - mse: 6253.3706 - val_loss: 4760.4463 - val_mae: 48.0570 - val_mape: 545.2001 - val_mse: 4760.4463 - 79s/epoch - 34ms/step\n",
      "Epoch 2/100\n",
      "2293/2293 - 75s - loss: 4582.9292 - mae: 46.0639 - mape: 378.8862 - mse: 4582.9292 - val_loss: 3740.9082 - val_mae: 40.4888 - val_mape: 352.0985 - val_mse: 3740.9082 - 75s/epoch - 33ms/step\n",
      "Epoch 3/100\n",
      "2293/2293 - 75s - loss: 3870.7031 - mae: 44.0499 - mape: 329.8671 - mse: 3870.7031 - val_loss: 3473.9697 - val_mae: 39.6839 - val_mape: 340.5815 - val_mse: 3473.9697 - 75s/epoch - 33ms/step\n",
      "Epoch 4/100\n",
      "2293/2293 - 73s - loss: 3712.1409 - mae: 43.3326 - mape: 310.2152 - mse: 3712.1409 - val_loss: 3211.7083 - val_mae: 37.4755 - val_mape: 296.7780 - val_mse: 3211.7083 - 73s/epoch - 32ms/step\n",
      "Epoch 5/100\n",
      "2293/2293 - 73s - loss: 3385.3174 - mae: 41.0372 - mape: 265.5164 - mse: 3385.3174 - val_loss: 3329.7466 - val_mae: 36.8026 - val_mape: 177.3911 - val_mse: 3329.7466 - 73s/epoch - 32ms/step\n",
      "Epoch 6/100\n",
      "2293/2293 - 73s - loss: 3185.4155 - mae: 39.8710 - mape: 259.5807 - mse: 3185.4155 - val_loss: 2782.5872 - val_mae: 33.6831 - val_mape: 209.8325 - val_mse: 2782.5872 - 73s/epoch - 32ms/step\n",
      "Epoch 7/100\n",
      "2293/2293 - 73s - loss: 2921.2017 - mae: 37.7681 - mape: 235.5672 - mse: 2921.2017 - val_loss: 2690.1003 - val_mae: 33.1453 - val_mape: 182.9664 - val_mse: 2690.1003 - 73s/epoch - 32ms/step\n",
      "Epoch 8/100\n",
      "2293/2293 - 73s - loss: 2748.4170 - mae: 36.1193 - mape: 223.8681 - mse: 2748.4170 - val_loss: 2531.4033 - val_mae: 32.0920 - val_mape: 194.6411 - val_mse: 2531.4033 - 73s/epoch - 32ms/step\n",
      "Epoch 9/100\n",
      "2293/2293 - 73s - loss: 2632.7322 - mae: 35.2778 - mape: 217.2301 - mse: 2632.7322 - val_loss: 2458.8257 - val_mae: 31.1442 - val_mape: 191.0760 - val_mse: 2458.8257 - 73s/epoch - 32ms/step\n",
      "Epoch 10/100\n",
      "2293/2293 - 74s - loss: 2532.3369 - mae: 34.4454 - mape: 209.5487 - mse: 2532.3369 - val_loss: 2406.1887 - val_mae: 30.5456 - val_mape: 205.9035 - val_mse: 2406.1887 - 74s/epoch - 32ms/step\n",
      "Epoch 11/100\n",
      "2293/2293 - 73s - loss: 2451.2004 - mae: 33.9190 - mape: 205.1706 - mse: 2451.2004 - val_loss: 2318.0793 - val_mae: 30.0474 - val_mape: 231.4356 - val_mse: 2318.0793 - 73s/epoch - 32ms/step\n",
      "Epoch 12/100\n",
      "2293/2293 - 74s - loss: 2382.2864 - mae: 33.3481 - mape: 200.6050 - mse: 2382.2864 - val_loss: 2186.6782 - val_mae: 29.0902 - val_mape: 228.3642 - val_mse: 2186.6782 - 74s/epoch - 32ms/step\n",
      "Epoch 13/100\n",
      "2293/2293 - 74s - loss: 2334.0068 - mae: 32.9619 - mape: 197.6308 - mse: 2334.0068 - val_loss: 2288.2251 - val_mae: 29.6428 - val_mape: 215.6465 - val_mse: 2288.2251 - 74s/epoch - 32ms/step\n",
      "Epoch 14/100\n",
      "2293/2293 - 74s - loss: 2325.1323 - mae: 32.7643 - mape: 192.9114 - mse: 2325.1323 - val_loss: 2240.4475 - val_mae: 29.1849 - val_mape: 204.0338 - val_mse: 2240.4475 - 74s/epoch - 32ms/step\n",
      "Epoch 15/100\n",
      "2293/2293 - 74s - loss: 2249.6462 - mae: 32.1613 - mape: 193.7674 - mse: 2249.6462 - val_loss: 2233.5100 - val_mae: 29.2160 - val_mape: 196.9745 - val_mse: 2233.5100 - 74s/epoch - 32ms/step\n",
      "Epoch 16/100\n",
      "2293/2293 - 74s - loss: 2200.0706 - mae: 31.7642 - mape: 188.8872 - mse: 2200.0706 - val_loss: 2230.8523 - val_mae: 29.1508 - val_mape: 205.3354 - val_mse: 2230.8523 - 74s/epoch - 32ms/step\n",
      "Epoch 17/100\n",
      "2293/2293 - 75s - loss: 2169.8774 - mae: 31.5278 - mape: 189.2024 - mse: 2169.8774 - val_loss: 2159.2783 - val_mae: 28.9357 - val_mape: 203.6562 - val_mse: 2159.2783 - 75s/epoch - 33ms/step\n",
      "Epoch 18/100\n",
      "2293/2293 - 74s - loss: 2120.6135 - mae: 31.1141 - mape: 183.3501 - mse: 2120.6135 - val_loss: 2182.5994 - val_mae: 28.9224 - val_mape: 199.2372 - val_mse: 2182.5994 - 74s/epoch - 32ms/step\n",
      "Epoch 19/100\n",
      "2293/2293 - 75s - loss: 2088.5557 - mae: 30.7840 - mape: 183.1007 - mse: 2088.5557 - val_loss: 2158.5667 - val_mae: 28.9482 - val_mape: 197.6286 - val_mse: 2158.5667 - 75s/epoch - 33ms/step\n",
      "Epoch 20/100\n",
      "2293/2293 - 75s - loss: 2062.2747 - mae: 30.6029 - mape: 182.0762 - mse: 2062.2747 - val_loss: 2176.0720 - val_mae: 29.4658 - val_mape: 200.6304 - val_mse: 2176.0720 - 75s/epoch - 33ms/step\n",
      "Epoch 21/100\n",
      "2293/2293 - 74s - loss: 2044.8885 - mae: 30.4609 - mape: 182.8168 - mse: 2044.8885 - val_loss: 2198.2585 - val_mae: 29.6058 - val_mape: 190.6816 - val_mse: 2198.2585 - 74s/epoch - 32ms/step\n",
      "Epoch 22/100\n",
      "2293/2293 - 75s - loss: 2021.3966 - mae: 30.1884 - mape: 183.7673 - mse: 2021.3966 - val_loss: 2222.3198 - val_mae: 29.2947 - val_mape: 184.0117 - val_mse: 2222.3198 - 75s/epoch - 33ms/step\n",
      "Epoch 23/100\n",
      "2293/2293 - 75s - loss: 2036.1050 - mae: 30.3447 - mape: 182.2954 - mse: 2036.1050 - val_loss: 2221.6865 - val_mae: 29.2240 - val_mape: 183.7346 - val_mse: 2221.6865 - 75s/epoch - 33ms/step\n",
      "Epoch 24/100\n",
      "2293/2293 - 75s - loss: 2016.6951 - mae: 30.1537 - mape: 180.0201 - mse: 2016.6951 - val_loss: 2230.7966 - val_mae: 29.2877 - val_mape: 189.3613 - val_mse: 2230.7966 - 75s/epoch - 33ms/step\n",
      "Epoch 25/100\n",
      "2293/2293 - 75s - loss: 1995.9080 - mae: 29.9207 - mape: 179.0621 - mse: 1995.9080 - val_loss: 2250.8298 - val_mae: 29.3175 - val_mape: 184.6420 - val_mse: 2250.8298 - 75s/epoch - 33ms/step\n",
      "Epoch 26/100\n",
      "2293/2293 - 76s - loss: 1964.3796 - mae: 29.6837 - mape: 174.6613 - mse: 1964.3796 - val_loss: 2314.1555 - val_mae: 29.6118 - val_mape: 184.7099 - val_mse: 2314.1555 - 76s/epoch - 33ms/step\n",
      "Epoch 27/100\n",
      "2293/2293 - 76s - loss: 1944.6212 - mae: 29.6158 - mape: 175.2746 - mse: 1944.6212 - val_loss: 2310.9434 - val_mae: 29.8317 - val_mape: 190.4581 - val_mse: 2310.9434 - 76s/epoch - 33ms/step\n",
      "Epoch 28/100\n",
      "2293/2293 - 76s - loss: 1949.3469 - mae: 29.6883 - mape: 175.4852 - mse: 1949.3469 - val_loss: 2341.8599 - val_mae: 29.9565 - val_mape: 186.1233 - val_mse: 2341.8599 - 76s/epoch - 33ms/step\n",
      "Epoch 29/100\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "2293/2293 - 76s - loss: 1945.1240 - mae: 29.6723 - mape: 174.7159 - mse: 1945.1240 - val_loss: 2379.8203 - val_mae: 30.1751 - val_mape: 183.0962 - val_mse: 2379.8203 - 76s/epoch - 33ms/step\n",
      "Epoch 29: early stopping\n",
      "727/727 [==============================] - 7s 9ms/step - loss: 2730.8799 - mae: 33.9508 - mape: 213.7373 - mse: 2730.8799\n",
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-15/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./5_hyperparameters_3_models/model-15/model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000023C3B4AEB80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "for num, activation, recurrent_activation, dropout, recurrent_dropout, kernel_regularizer, recurrent_regularizer in cases[3:]:\n",
    "    print(num, activation, recurrent_activation, dropout, recurrent_dropout, kernel_regularizer, recurrent_regularizer)\n",
    "\n",
    "    kernel_regularizer_obj = None\n",
    "    if kernel_regularizer == 'l1':\n",
    "        kernel_regularizer_obj = regularizers.L1()\n",
    "    elif kernel_regularizer == 'l2':\n",
    "        kernel_regularizer_obj = regularizers.L2()\n",
    "    elif kernel_regularizer == 'l1l2':\n",
    "        kernel_regularizer_obj = regularizers.L1L2()\n",
    "    \n",
    "    recurrent_regularizer_obj = None\n",
    "    if recurrent_regularizer == 'l1':\n",
    "        recurrent_regularizer_obj = regularizers.L1()\n",
    "    elif recurrent_regularizer == 'l2':\n",
    "        recurrent_regularizer_obj = regularizers.L2()\n",
    "    elif recurrent_regularizer == 'l1l2':\n",
    "        recurrent_regularizer_obj = regularizers.L1L2()\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.LSTM(\n",
    "            32, \n",
    "            input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), \n",
    "            activation=activation,\n",
    "            recurrent_activation=recurrent_activation,\n",
    "            dropout=dropout, \n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            kernel_regularizer=kernel_regularizer_obj,\n",
    "            recurrent_regularizer=recurrent_regularizer_obj\n",
    "        ),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape', 'mse'])\n",
    "\n",
    "    model_history = model.fit(\n",
    "        X_train_seq, \n",
    "        y_train_seq, \n",
    "        epochs=100, \n",
    "        batch_size=1024, \n",
    "        validation_data=(X_val_seq, y_val_seq), \n",
    "        shuffle=False,\n",
    "        verbose=2,\n",
    "        callbacks=[\n",
    "            callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\", verbose=1, restore_best_weights=True)]\n",
    "        )\n",
    "\n",
    "    model_test_eval = model.evaluate(X_test_seq, y_test_seq, batch_size=1024)\n",
    "    \n",
    "    name = f'model-{num}'\n",
    "    save_results(model, model_history.history, model_test_eval, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fac39324d9198cc76e2485f4289bad7daf596360a39b38c5d465d42e309c1b61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
